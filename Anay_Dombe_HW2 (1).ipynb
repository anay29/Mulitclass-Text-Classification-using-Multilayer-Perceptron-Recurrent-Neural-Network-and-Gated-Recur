{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osSnTOUH_rWB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "#nltk.download('wordnet')\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import sys\n",
        "# !pip install contractions\n",
        "# !pip uninstall torchtext\n",
        "# !pip install torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DNo_dzBCqQe",
        "outputId": "c65c1879-aa80-4ce5-8eaf-c674918d67d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "5_Wf-hgMALya",
        "outputId": "f3e48664-1fe1-425d-f194-2ed4503294a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
              "0          US     50423057  R135Q3VZ4DQN5N  B00JWXFDMG       657335467   \n",
              "1          US     11262325  R2N0QQ6R4T7YRY  B00W5T1H9W        26030170   \n",
              "2          US     27541121  R3N5JE5Y4T6W5M  B00M2L6KFY       697845240   \n",
              "3          US      5350721  R2I150CX5IVY9Q  B0006SW2WU       569859289   \n",
              "4          US     24484424  R1RM9ICOOA9MQ3  B009YPDW70       332947422   \n",
              "\n",
              "                                       product_title product_category  \\\n",
              "0  Everbling Purple and Clear Briolette Drop Swar...          Jewelry   \n",
              "1  925 Sterling Silver Finish 6ct Simulated Diamo...          Jewelry   \n",
              "2  Sterling Silver Circle \"Friends Forever\" Infin...          Jewelry   \n",
              "3  Surgical Stainless Steel Domed 9mm Fishbone Ri...          Jewelry   \n",
              "4       Sterling Silver Family Pendant Necklace, 18\"          Jewelry   \n",
              "\n",
              "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
              "0           5            0.0          0.0    N                 Y   \n",
              "1           5            0.0          0.0    N                 N   \n",
              "2           5            0.0          0.0    N                 Y   \n",
              "3           5            0.0          0.0    N                 Y   \n",
              "4           5            0.0          0.0    N                 Y   \n",
              "\n",
              "                                     review_headline  \\\n",
              "0                                          Beauties!   \n",
              "1                                     Great product.   \n",
              "2  Exactly as pictured and my daughter's friend l...   \n",
              "3                                         Five Stars   \n",
              "4  ... a Mother's Day gift for my Mom and she lov...   \n",
              "\n",
              "                                         review_body review_date  \n",
              "0  so beautiful even tho clearly not high end ......  2015-08-31  \n",
              "1  Great product.. I got this set for my mother, ...  2015-08-31  \n",
              "2  Exactly as pictured and my daughter's friend l...  2015-08-31  \n",
              "3  Love it. Fits great. Super comfortable and nea...  2015-08-31  \n",
              "4  Got this as a Mother's Day gift for my Mom and...  2015-08-31  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d2f8329-355c-4984-8007-95510c1c3262\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>marketplace</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_parent</th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US</td>\n",
              "      <td>50423057</td>\n",
              "      <td>R135Q3VZ4DQN5N</td>\n",
              "      <td>B00JWXFDMG</td>\n",
              "      <td>657335467</td>\n",
              "      <td>Everbling Purple and Clear Briolette Drop Swar...</td>\n",
              "      <td>Jewelry</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Beauties!</td>\n",
              "      <td>so beautiful even tho clearly not high end ......</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>11262325</td>\n",
              "      <td>R2N0QQ6R4T7YRY</td>\n",
              "      <td>B00W5T1H9W</td>\n",
              "      <td>26030170</td>\n",
              "      <td>925 Sterling Silver Finish 6ct Simulated Diamo...</td>\n",
              "      <td>Jewelry</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Great product.</td>\n",
              "      <td>Great product.. I got this set for my mother, ...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US</td>\n",
              "      <td>27541121</td>\n",
              "      <td>R3N5JE5Y4T6W5M</td>\n",
              "      <td>B00M2L6KFY</td>\n",
              "      <td>697845240</td>\n",
              "      <td>Sterling Silver Circle \"Friends Forever\" Infin...</td>\n",
              "      <td>Jewelry</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
              "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US</td>\n",
              "      <td>5350721</td>\n",
              "      <td>R2I150CX5IVY9Q</td>\n",
              "      <td>B0006SW2WU</td>\n",
              "      <td>569859289</td>\n",
              "      <td>Surgical Stainless Steel Domed 9mm Fishbone Ri...</td>\n",
              "      <td>Jewelry</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US</td>\n",
              "      <td>24484424</td>\n",
              "      <td>R1RM9ICOOA9MQ3</td>\n",
              "      <td>B009YPDW70</td>\n",
              "      <td>332947422</td>\n",
              "      <td>Sterling Silver Family Pendant Necklace, 18\"</td>\n",
              "      <td>Jewelry</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>... a Mother's Day gift for my Mom and she lov...</td>\n",
              "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d2f8329-355c-4984-8007-95510c1c3262')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9d2f8329-355c-4984-8007-95510c1c3262 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9d2f8329-355c-4984-8007-95510c1c3262');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#amazon_reviews_data=pd.read_csv(os.path.join(sys.path[0], 'amazon_reviews_us_Jewelry_v1_00.tsv'),sep='\\t',on_bad_lines='skip')\n",
        "# stored in /content/gdrive/mydrive/colab notebooks in google colab\n",
        "amazon_reviews_data=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/amazon_reviews_us_Jewelry_v1_00.tsv',sep='\\t',on_bad_lines='skip')\n",
        "amazon_reviews_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xU92iWYZGFvU",
        "outputId": "bcce080f-dd34-48b6-acd5-53ccb492fcd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  star_rating                                        review_body\n",
              "0           5  so beautiful even tho clearly not high end ......\n",
              "1           5  Great product.. I got this set for my mother, ...\n",
              "2           5  Exactly as pictured and my daughter's friend l...\n",
              "3           5  Love it. Fits great. Super comfortable and nea...\n",
              "4           5  Got this as a Mother's Day gift for my Mom and..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebdd44bd-0ed3-4859-b5d4-fcd2701e33b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>star_rating</th>\n",
              "      <th>review_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>so beautiful even tho clearly not high end ......</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>Great product.. I got this set for my mother, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebdd44bd-0ed3-4859-b5d4-fcd2701e33b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ebdd44bd-0ed3-4859-b5d4-fcd2701e33b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ebdd44bd-0ed3-4859-b5d4-fcd2701e33b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "ratings_reviews_data=amazon_reviews_data[['star_rating','review_body']]\n",
        "ratings_reviews_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYG8yFDtCA12"
      },
      "source": [
        "**We select 20000 reviews randomly from each rating class to create a balanced dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVdoIqHWCHyt",
        "outputId": "35009a0b-68ec-4e81-bb15-3a4a904f0039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Ratings before cleaning\n",
            "\n",
            "[5 1 4 3 2 nan '4' '5' '2' '3' '1' '2012-12-21']\n",
            "\n",
            "\n",
            "Unique Ratings after cleaning:\n",
            "\n",
            "[5 1 4 3 2]\n",
            "\n",
            "Count of number of values for each rating\n",
            "\n",
            "5    1080871\n",
            "4     270424\n",
            "3     159654\n",
            "1     155002\n",
            "2     100797\n",
            "Name: star_rating, dtype: int64\n",
            "\n",
            " Final Count of number of values for each rating keeping 20k per each rating\n",
            "\n",
            "5    10000\n",
            "1    10000\n",
            "4    10000\n",
            "3    10000\n",
            "2    10000\n",
            "Name: star_rating, dtype: int64\n",
            "       star_rating                                        review_body\n",
            "0                5  so beautiful even tho clearly not high end ......\n",
            "1                5  Great product.. I got this set for my mother, ...\n",
            "2                5  Exactly as pictured and my daughter's friend l...\n",
            "3                5  Love it. Fits great. Super comfortable and nea...\n",
            "4                5  Got this as a Mother's Day gift for my Mom and...\n",
            "...            ...                                                ...\n",
            "49995            2  Poor quality. Get what you pay for in this cas...\n",
            "49996            2  This perfume confuses me so much. I really wan...\n",
            "49997            2                                         Way to big\n",
            "49998            2  The necklace and very fragile and broke fast, ...\n",
            "49999            2  i bought this ring under the impression that i...\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print (\"Unique Ratings before cleaning\\n\")\n",
        "print (ratings_reviews_data['star_rating'].unique())\n",
        "print (\"\\n\")\n",
        "pd.options.mode.chained_assignment = None\n",
        "# Above statement shows that there are some strings which have same value as int and some values like nan, date in star rating columns which need to be dropped\n",
        "\n",
        "\n",
        "ratings_reviews_data['star_rating'] = pd.to_numeric(ratings_reviews_data['star_rating'], errors='coerce').fillna(0).astype(np.int64) # converting all values to int \n",
        "\n",
        "ratings_reviews_data = ratings_reviews_data[ratings_reviews_data['star_rating'].notna()] # keeping only those ratings which are not nan\n",
        "ratings_reviews_data = ratings_reviews_data[ratings_reviews_data['review_body'].notna()] # keeping only those reviews which are not nan\n",
        "#ratings_reviews_data = ratings_reviews_data[ratings_reviews_data['review_headline'].notna()] # keeping only those reviews which are not nan\n",
        "\n",
        "print (\"Unique Ratings after cleaning:\\n\")\n",
        "print (ratings_reviews_data['star_rating'].unique())\n",
        "\n",
        "# We cannot simply remove all nan since there maybe nans in only date columns which we will drop later\n",
        "\n",
        "print (\"\\nCount of number of values for each rating\\n\")\n",
        "print (ratings_reviews_data['star_rating'].value_counts())\n",
        "\n",
        "#n = 20000\n",
        "n=5000\n",
        "\n",
        "#ratings_sampled_data=ratings_reviews_data.groupby('star_rating').apply(lambda x: x.sample(min(n,len(x)))).reset_index(drop=True) # this line of code should be used for chosing random samples\n",
        "\n",
        "\n",
        "ratings_sampled_data=ratings_reviews_data.groupby('star_rating').head(10000).reset_index(drop=True) # to be consistent with the python file use this line of code \n",
        "# In the above statement I have selected 12000 from each class since the session was crashing when selecting all 20000. I have indiividually tried for 20000 points and results improve than current results\n",
        "\n",
        "\n",
        "\n",
        "print (\"\\n Final Count of number of values for each rating keeping 20k per each rating\\n\")\n",
        "print (ratings_sampled_data['star_rating'].value_counts())\n",
        "\n",
        "# y=ratings_sampled_data['star_rating']\n",
        "# X=ratings_sampled_data.drop('star_rating',axis=1)\n",
        "\n",
        "#  from sklearn.model_selection import train_test_split\n",
        "#  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # split the data\n",
        "\n",
        "print (ratings_sampled_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR7GjYVHGkum"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "20J-l85YGmz_",
        "outputId": "edfec03a-71a6-48e7-e1d9-7172b1680c34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       star_rating                                        review_body\n",
              "0                5  so beautiful even though clearly not high end ...\n",
              "1                5  great product i got this set for my mother as ...\n",
              "2                5  exactly as pictured and my daughters friend lo...\n",
              "3                5  love it fits great super comfortable and neat ...\n",
              "4                5  got this as a mothers day gift for my mom and ...\n",
              "...            ...                                                ...\n",
              "49995            2  poor quality get what you pay for in this case...\n",
              "49996            2  this perfume confuses me so much i really want...\n",
              "49997            2                                         way to big\n",
              "49998            2  the necklace and very fragile and broke fast t...\n",
              "49999            2  i bought this ring under the impression that i...\n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-faca4fe7-c8e7-4811-a984-65a5a31c7d6e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>star_rating</th>\n",
              "      <th>review_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>so beautiful even though clearly not high end ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>great product i got this set for my mother as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>exactly as pictured and my daughters friend lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>love it fits great super comfortable and neat ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>got this as a mothers day gift for my mom and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>2</td>\n",
              "      <td>poor quality get what you pay for in this case...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>2</td>\n",
              "      <td>this perfume confuses me so much i really want...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>2</td>\n",
              "      <td>way to big</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>2</td>\n",
              "      <td>the necklace and very fragile and broke fast t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>2</td>\n",
              "      <td>i bought this ring under the impression that i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faca4fe7-c8e7-4811-a984-65a5a31c7d6e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-faca4fe7-c8e7-4811-a984-65a5a31c7d6e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-faca4fe7-c8e7-4811-a984-65a5a31c7d6e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import contractions\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^a-z #+_]')\n",
        "\n",
        "def txt_cleaning(text):\n",
        "    text = contractions.fix(text)\n",
        "    text = text.lower() # converting all reviews to lowercase\n",
        "    text = BeautifulSoup(text, \"lxml\").text # removing HTML tags and URL's\n",
        "    text = text.replace(r's*https?://S+(s+|$)', ' ').strip()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    return text\n",
        "\n",
        "ratings_sampled_data['review_body'] = ratings_sampled_data['review_body'].apply(txt_cleaning)\n",
        "ratings_sampled_data['review_body']=  ratings_sampled_data['review_body'].replace({' +':' '},regex=True)\n",
        "ratings_sampled_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Only basic data cleaning has been performed such as removing tags, links or other bad charcaters. I am not removing stop words.**"
      ],
      "metadata": {
        "id": "azj388AfuiQl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE0SN5AgG33e"
      },
      "source": [
        "**We need to split the data into train and test first in order to avoid leakage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZyn9ac3G94V"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = ratings_sampled_data.iloc[:, 1].values\n",
        "y = ratings_sampled_data.iloc[:, 0].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpm16ok9I09k"
      },
      "source": [
        "**Word Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNgHc9_VI6ZX"
      },
      "source": [
        "**Loading the google new pretrained embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_TgxeluGKbF"
      },
      "outputs": [],
      "source": [
        "# import gensim.downloader as api\n",
        "# model = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpWElzKuI-w5"
      },
      "source": [
        "**Saving the pretrained embeddings in order to save the time to reload them again**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs98nYPw68Rk"
      },
      "outputs": [],
      "source": [
        "# model.init_sims(replace=True)\n",
        "\n",
        "# model.save(\"/content/gdrive/MyDrive/Colab Notebooks/stored_pretrained_google_model.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUavQsMNJIqR"
      },
      "source": [
        "**Checking the semantic similarities for the pretrained embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7j4XYPp8s7t"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from threading import Semaphore\n",
        "model = KeyedVectors.load('/content/gdrive/MyDrive/Colab Notebooks/stored_pretrained_google_model.model', mmap='r')\n",
        "#model.syn0norm = model.syn0  # prevent recalc of normed vectors\n",
        "#Semaphore(0).acquire()  # just hang until process killed\n",
        "#model.most_similar(positive=['woman','king'], negative=['man'], topn = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P-wTq6lHiR8",
        "outputId": "7c96fc10-1eae-42b7-c017-96bc5f92ee36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('maroon', 0.6170071363449097),\n",
              " ('burgundy', 0.5899689793586731),\n",
              " ('red', 0.5830079913139343),\n",
              " ('crimson', 0.5826750993728638),\n",
              " ('yellow_plaid_pants', 0.5572718381881714)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar(positive=['purple','blue'], negative=['pink'], topn = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIcSfaeCH64y",
        "outputId": "94920dea-8cba-496a-f34f-2cf26ca262b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('wife', 0.7284117937088013),\n",
              " ('son', 0.7059542536735535),\n",
              " ('mother', 0.6460014581680298),\n",
              " ('father', 0.6453131437301636),\n",
              " ('fiance', 0.6444408893585205)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar(positive=['daughter','husband'], negative=['sister'], topn = 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(positive=['ring','locket'], negative=['finger'], topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEXLVYVFtGAn",
        "outputId": "53df55b8-0ebc-400e-fef2-69fc64840608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('necklace', 0.5243216753005981),\n",
              " ('rings', 0.5141297578811646),\n",
              " ('pendant', 0.4835904836654663),\n",
              " ('locket_necklace', 0.45760393142700195),\n",
              " ('jade_bracelet', 0.45525795221328735)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looking at the first semantic similarity, we are feeding 3 colors so naturally the most similar choice should be a color. Similarly in the second example, wife is most similar to husband since husband and wife occur together mostly in general. In the third example, ring and finger are related so locket and necklace are also related.**"
      ],
      "metadata": {
        "id": "7QM1Mq-1vEqj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjtr3FB3JSaK"
      },
      "source": [
        "**Training Word2Vec model using our dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "SnbQmxb0SFs1",
        "outputId": "e13244e1-b9c9-4db2-f52d-4c00290fcc9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             review_body\n",
              "0                                        completely fake\n",
              "1                                     very happy with it\n",
              "2      it is bigger than i was expecting it almost to...\n",
              "3                       color change after wearing twice\n",
              "4      bought this and after about a while it started...\n",
              "...                                                  ...\n",
              "47995  i wore this chain three times and then it brok...\n",
              "47996                                        its too big\n",
              "47997  feel apart right after she put it on my mom lo...\n",
              "47998     looks cute but tickles my nose too bad to wear\n",
              "47999                                            love it\n",
              "\n",
              "[48000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b43670e-640c-4f07-8bb8-cbd1731577a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>completely fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>very happy with it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it is bigger than i was expecting it almost to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>color change after wearing twice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bought this and after about a while it started...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47995</th>\n",
              "      <td>i wore this chain three times and then it brok...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47996</th>\n",
              "      <td>its too big</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47997</th>\n",
              "      <td>feel apart right after she put it on my mom lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47998</th>\n",
              "      <td>looks cute but tickles my nose too bad to wear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47999</th>\n",
              "      <td>love it</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b43670e-640c-4f07-8bb8-cbd1731577a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b43670e-640c-4f07-8bb8-cbd1731577a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b43670e-640c-4f07-8bb8-cbd1731577a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "X_tr=pd.DataFrame(columns=['review_body'])\n",
        "y_tr=pd.DataFrame(columns=['star_rating'])\n",
        "X_ts=pd.DataFrame(columns=['review_body'])\n",
        "y_ts=pd.DataFrame(columns=['star_rating'])\n",
        "X_tr['review_body']=X_train\n",
        "X_ts['review_body']=X_test\n",
        "y_tr['star_rating']=y_train\n",
        "y_ts['star_rating']=y_test\n",
        "X_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s4btH8OIWUz"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "training_corpus = X_tr.review_body.apply(lambda x: x.split(\" \"))  # splitting all the words into tokens since word2vec expects list of strings\n",
        "w2v_model= Word2Vec(sentences=training_corpus, size=300, window=11, min_count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQt_v3tRT7jc",
        "outputId": "2c22266d-5076-4b54-fdcc-05089c8f5cc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('fianc', 0.784647524356842),\n",
              " ('son', 0.7840499877929688),\n",
              " ('hubby', 0.7766162157058716),\n",
              " ('boyfriend', 0.7722760438919067),\n",
              " ('wife', 0.7680268883705139)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v_model.wv.most_similar(positive=['daughter','husband'], negative=['sister'], topn = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adgxszbLW_xM",
        "outputId": "ce694fa7-d75e-4fba-8864-2f429aec9786"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('sapphire', 0.8536776900291443),\n",
              " ('topaz', 0.8481096029281616),\n",
              " ('deep', 0.8479543924331665),\n",
              " ('amethyst', 0.8239973187446594),\n",
              " ('red', 0.8147281408309937)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v_model.wv.most_similar(positive=['purple','blue'], negative=['pink'], topn = 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=['ring','locket'], negative=['finger'], topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSboFgAnv4K5",
        "outputId": "c31adacc-62a7-430e-a41d-d4cc865511a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('necklace', 0.6700347661972046),\n",
              " ('pin', 0.6254616379737854),\n",
              " ('pendant', 0.6015681028366089),\n",
              " ('item', 0.5900734663009644),\n",
              " ('cross', 0.5855146050453186)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tmD8s9wXM-M"
      },
      "source": [
        "**Pretrained word embeddings seem to show more similar results due to huge vocabulary. Word2vec might sometimes also not find a word in the vocabulary. But in case of our 3 examples both are perfroming well. There might be case when the dataset does not have enough words which is when word2vec might not give good similarities. On the other hand sometimes word2vec might perform better if there are words in the dataset which need to go together. For ex, in a certain data husband and son might be more related than husband and wife so in this case word2vec might perform better**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xks7qJSGLfx"
      },
      "outputs": [],
      "source": [
        "# !pip install torchtext==0.9.1\n",
        "import torchtext.legacy\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "tokenizer = lambda s: s.split() # tokenize the reviews\n",
        "\n",
        "Text = data.Field(tokenize=tokenizer, batch_first=True, include_lengths=True)\n",
        "Label = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
        "\n",
        "fields = [('star_rating', Label),('review_body', Text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn226Q6RW0NZ"
      },
      "outputs": [],
      "source": [
        "# We need to convert the pandas dataframe to torchtext dataset\n",
        "\n",
        "list_of_examples = [data.Example.fromlist(row.tolist(),fields=fields) for _, row in ratings_sampled_data.iterrows()]  #converting each row of dataframe to torch text example which cintains text and label fields\n",
        "\n",
        "torch_data = data.Dataset(examples=list_of_examples, fields=fields)  #constructing the torchtext dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwccthYuAv6s"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "training_data, testing_data = torch_data.split(split_ratio=0.8, random_state=random.seed(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-8Jynuw6TXz"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data = training_data.split(split_ratio=0.8, random_state=random.seed(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR-QXSlGCWH0"
      },
      "source": [
        "**Building the vocabulary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX5RcWFCJn9f"
      },
      "source": [
        "**Loading an embedding layer with pretrained google news and bulding the vocabulary using pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWNN0QSJDUDd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import vocab\n",
        "# Text.build_vocab(train_data, validation_data) \n",
        "# word2index = {token: token_index for token_index, token in enumerate(model.index2word)}\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Text.vocab.set_vectors(word2index, torch.from_numpy(model.vectors).float().to(device), model.vector_size)\n",
        "# Label.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In all the 3 cells below, I am checking if the word from our data is present in the google pretrained vectors or not. If yes, then I am appending else it will be encoded with zeros of dimension 300 since the dimension of each word is 300. Then we calculate the average of all the appended words. This will be done for training data, validation data and testing data.**"
      ],
      "metadata": {
        "id": "SBQ9eV1rxh8Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34a1y50V8dhC"
      },
      "outputs": [],
      "source": [
        "x_training=[]\n",
        "y_training=[]\n",
        "for i in range(len(training_data)):\n",
        "  doc=[]\n",
        "  for word in vars(training_data[i])['review_body']:\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "  if (len(doc)!=0):\n",
        "    x_training.append(np.mean(doc, axis=0))\n",
        "    y_training.append(vars(training_data[i])['star_rating'])\n",
        "  else:\n",
        "    x_training.append(np.zeros(300))\n",
        "    y_training.append(vars(training_data[i])['star_rating'])\n",
        "\n",
        "X_training_simple_avg = np.array(x_training)\n",
        "y_training_simple_avg = np.array(y_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PArJn3_Qibf-"
      },
      "outputs": [],
      "source": [
        "x_tr=[]\n",
        "y_tr=[]\n",
        "x_ts=[]\n",
        "y_ts=[]\n",
        "for i in range(len(train_data)):\n",
        "  doc=[]\n",
        "  for word in vars(train_data[i])['review_body']:\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "  if (len(doc)!=0):\n",
        "    x_tr.append(np.mean(doc, axis=0))\n",
        "    y_tr.append(vars(train_data[i])['star_rating'])\n",
        "  else:\n",
        "    x_tr.append(np.zeros(300))\n",
        "    y_tr.append(vars(train_data[i])['star_rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr5ZFnB18BfE"
      },
      "outputs": [],
      "source": [
        "x_val=[]\n",
        "y_val=[]\n",
        "for i in range(len(validation_data)):\n",
        "  doc=[]\n",
        "  for word in vars(validation_data[i])['review_body']:\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "  if (len(doc)!=0):\n",
        "    x_val.append(np.mean(doc, axis=0))\n",
        "    y_val.append(vars(validation_data[i])['star_rating'])\n",
        "  else:\n",
        "    x_val.append(np.zeros(300))\n",
        "    y_val.append(vars(validation_data[i])['star_rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEDwRqX7qSmf"
      },
      "outputs": [],
      "source": [
        "for i in range(len(testing_data)):\n",
        "  doc=[]\n",
        "  for word in vars(testing_data[i])['review_body']:\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "  if (len(doc)!=0):\n",
        "    x_ts.append(np.mean(doc, axis=0))\n",
        "    y_ts.append(vars(testing_data[i])['star_rating'])\n",
        "  else:\n",
        "    x_ts.append(np.zeros(300))\n",
        "    y_ts.append(vars(testing_data[i])['star_rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating results using SVM and average vector as features**"
      ],
      "metadata": {
        "id": "d_bhmE-Dzz7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3-n-c0AcHza",
        "outputId": "164d216c-5a52-4a09-9ac8-3d69ff825e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.51      0.72      0.59      2361\n",
            "           2       0.40      0.26      0.32      2391\n",
            "           3       0.41      0.39      0.40      2451\n",
            "           4       0.42      0.32      0.36      2377\n",
            "           5       0.60      0.75      0.67      2420\n",
            "\n",
            "    accuracy                           0.49     12000\n",
            "   macro avg       0.47      0.49      0.47     12000\n",
            "weighted avg       0.47      0.49      0.47     12000\n",
            "\n",
            "Accuracy: 0.49\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_tr_simple_avg = np.array(x_tr)\n",
        "X_val_simple_avg = np.array(x_val)\n",
        "X_ts_simple_avg = np.array(x_ts)\n",
        "y_tr_simple_avg = np.array(y_tr)\n",
        "y_val_simple_avg = np.array(y_val)\n",
        "y_ts_simple_avg = np.array(y_ts)\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]} \n",
        "\n",
        "#svm_model = GridSearchCV(LinearSVC(), param_grid, refit = True, verbose = 0, scoring='f1_micro')\n",
        "\n",
        "svm_model=LinearSVC() \n",
        "\n",
        "svm_model.fit(X_training_simple_avg, y_training_simple_avg)\n",
        "y_pred_simple = svm_model.predict(X_ts_simple_avg)\n",
        "\n",
        "print (classification_report(y_ts_simple_avg, y_pred_simple))\n",
        "print('Accuracy: {:.2f}'.format(accuracy_score(y_ts_simple_avg, y_pred_simple)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results for SVM taking TFIDF (copied from HW1 directly as per the post https://piazza.com/class/l7102doc7aa3ob/post/223)\n",
        "\n",
        "           1       0.57      0.71      0.64      4029\n",
        "           2       0.44      0.35      0.39      3977\n",
        "           3       0.45      0.40      0.42      3981\n",
        "           4       0.49      0.43      0.46      3993\n",
        "           5       0.65      0.78      0.70      4020\n",
        "\n",
        "    accuracy                           0.53     20000\n"
      ],
      "metadata": {
        "id": "bb7Ix7hI009M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From above we can say that using TF-IDF has given us better results for SVM than using average vectors by 4 percent. This might be because when we are using average as a feature, there might some information loss as to not giving more weight to important words in the corpus. Also, the average will just be 1 300 dim vector containing weight of all the words in the review. But the average might vary depending on the word present in the corpus. If the word is less important and is being used in average, it might have an impact on the embedding values. In TFIDF, however since it depends on the frequency, model might be peprforming better. However, there might be cases when average might perform better because we are using pre trained embeddings which are based on context. Accuracy for this SVM can also be improved by hyperparameter tuning.**"
      ],
      "metadata": {
        "id": "KErGwMBa1q6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Perceptron using average as a feature**"
      ],
      "metadata": {
        "id": "55RNS-Qf5Vak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CeYYSvO10Ru",
        "outputId": "4697bd3c-87ca-4376-d1cb-65b7ee60be95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.54      0.55      0.54      2361\n",
            "           2       0.28      0.70      0.40      2391\n",
            "           3       0.61      0.02      0.03      2451\n",
            "           4       0.44      0.10      0.17      2377\n",
            "           5       0.60      0.75      0.66      2420\n",
            "\n",
            "    accuracy                           0.42     12000\n",
            "   macro avg       0.49      0.42      0.36     12000\n",
            "weighted avg       0.49      0.42      0.36     12000\n",
            "\n",
            "Accuracy: 0.42\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "per_model=Perceptron()\n",
        "\n",
        "per_model.fit(X_training_simple_avg, y_training_simple_avg)\n",
        "y_pred_simple = per_model.predict(X_ts_simple_avg)\n",
        "\n",
        "print (classification_report(y_ts_simple_avg, y_pred_simple))\n",
        "print('Accuracy: {:.2f}'.format(accuracy_score(y_ts_simple_avg, y_pred_simple)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple perceptron using TFIDF (copied from HW1 directly as per post https://piazza.com/class/l7102doc7aa3ob/post/223)**\n",
        "\n",
        "\n",
        "           1       0.65      0.29      0.40      4029\n",
        "           2       0.30      0.20      0.24      3977\n",
        "           3       0.26      0.68      0.38      3981\n",
        "           4       0.38      0.18      0.24      3993\n",
        "           5       0.61      0.49      0.55      4020\n",
        "\n",
        "    accuracy                           0.37     20000"
      ],
      "metadata": {
        "id": "opMG3hc05oXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From above, we can say that for perceptron, average as a feature is performing better than TFIDF. As mentioned above there might be cases when average might perform better because we are using pre trained embeddings which are based on context and perceptron might be better able to identify the context because of pretrained embeddings over TF-IDF. It also depends on the model and hyperparameter tuning.**"
      ],
      "metadata": {
        "id": "YVJT2Jk36fXD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aVnNrzYibFk"
      },
      "source": [
        "**Feedforward Neural Networks by taking the average of word vectors for a review**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yE0J_3tgL6l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.nn import Sigmoid\n",
        "from numpy import vstack\n",
        "class avgMLP(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(avgMLP, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "        self.hidden_size1=50\n",
        "        self.hidden_size2=10\n",
        "        self.output_dim=5  # since we have 5 class classification problem\n",
        "        self.hidden1 = torch.nn.Linear(input_size, self.hidden_size1)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.hidden2 = torch.nn.Linear(self.hidden_size1, self.hidden_size2)\n",
        "        self.act2 = torch.nn.ReLU()\n",
        "        self.hidden3 = torch.nn.Linear(self.hidden_size2, self.output_dim)\n",
        "        self.act3 = Sigmoid()\n",
        "        \n",
        "    def forward(self, X):\n",
        "        X = X.view(X.size(0), -1) \n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "        #X = self.dropout(X)\n",
        "         # second hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        #X = self.dropout(X)\n",
        "        X = self.hidden3(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgFEMk7A_LdH",
        "outputId": "54f8e58d-316b-4536-ff60-f5f4c7b6f880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.632 | Train Acc: 19.98%\n",
            "\t Val. Loss: 1.626 |  Val. Acc: 19.54%\n",
            "\tTrain Loss: 1.625 | Train Acc: 19.98%\n",
            "\t Val. Loss: 1.619 |  Val. Acc: 19.54%\n",
            "\tTrain Loss: 1.617 | Train Acc: 19.98%\n",
            "\t Val. Loss: 1.610 |  Val. Acc: 19.54%\n",
            "\tTrain Loss: 1.609 | Train Acc: 19.98%\n",
            "\t Val. Loss: 1.599 |  Val. Acc: 21.99%\n",
            "\tTrain Loss: 1.597 | Train Acc: 22.80%\n",
            "\t Val. Loss: 1.584 |  Val. Acc: 25.33%\n",
            "\tTrain Loss: 1.582 | Train Acc: 25.90%\n",
            "\t Val. Loss: 1.566 |  Val. Acc: 28.35%\n",
            "\tTrain Loss: 1.563 | Train Acc: 28.96%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 28.64%\n",
            "\tTrain Loss: 1.544 | Train Acc: 29.05%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 29.98%\n",
            "\tTrain Loss: 1.530 | Train Acc: 30.02%\n",
            "\t Val. Loss: 1.526 |  Val. Acc: 30.13%\n",
            "\tTrain Loss: 1.522 | Train Acc: 30.27%\n",
            "\t Val. Loss: 1.518 |  Val. Acc: 26.96%\n",
            "\tTrain Loss: 1.514 | Train Acc: 26.69%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 30.31%\n",
            "\tTrain Loss: 1.509 | Train Acc: 30.44%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 28.36%\n",
            "\tTrain Loss: 1.500 | Train Acc: 28.06%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 30.36%\n",
            "\tTrain Loss: 1.499 | Train Acc: 30.49%\n",
            "\t Val. Loss: 1.492 |  Val. Acc: 30.43%\n",
            "\tTrain Loss: 1.488 | Train Acc: 30.18%\n",
            "\t Val. Loss: 1.495 |  Val. Acc: 30.34%\n",
            "\tTrain Loss: 1.489 | Train Acc: 30.42%\n",
            "\t Val. Loss: 1.481 |  Val. Acc: 32.51%\n",
            "\tTrain Loss: 1.476 | Train Acc: 32.04%\n",
            "\t Val. Loss: 1.487 |  Val. Acc: 30.67%\n",
            "\tTrain Loss: 1.481 | Train Acc: 30.68%\n",
            "\t Val. Loss: 1.470 |  Val. Acc: 33.64%\n",
            "\tTrain Loss: 1.465 | Train Acc: 33.29%\n",
            "\t Val. Loss: 1.479 |  Val. Acc: 30.88%\n",
            "\tTrain Loss: 1.473 | Train Acc: 30.89%\n",
            "\t Val. Loss: 1.461 |  Val. Acc: 34.96%\n",
            "\tTrain Loss: 1.456 | Train Acc: 34.71%\n",
            "\t Val. Loss: 1.473 |  Val. Acc: 31.19%\n",
            "\tTrain Loss: 1.466 | Train Acc: 31.45%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 35.23%\n",
            "\tTrain Loss: 1.450 | Train Acc: 34.84%\n",
            "\t Val. Loss: 1.468 |  Val. Acc: 31.20%\n",
            "\tTrain Loss: 1.460 | Train Acc: 31.67%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 35.34%\n",
            "\tTrain Loss: 1.445 | Train Acc: 35.12%\n",
            "\t Val. Loss: 1.463 |  Val. Acc: 31.21%\n",
            "\tTrain Loss: 1.456 | Train Acc: 31.92%\n",
            "\t Val. Loss: 1.446 |  Val. Acc: 36.02%\n",
            "\tTrain Loss: 1.440 | Train Acc: 35.48%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 31.14%\n",
            "\tTrain Loss: 1.451 | Train Acc: 32.26%\n",
            "\t Val. Loss: 1.441 |  Val. Acc: 36.20%\n",
            "\tTrain Loss: 1.434 | Train Acc: 35.79%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 31.49%\n",
            "\tTrain Loss: 1.447 | Train Acc: 32.40%\n",
            "\t Val. Loss: 1.437 |  Val. Acc: 36.56%\n",
            "\tTrain Loss: 1.430 | Train Acc: 36.04%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 31.79%\n",
            "\tTrain Loss: 1.443 | Train Acc: 32.74%\n",
            "\t Val. Loss: 1.434 |  Val. Acc: 36.84%\n",
            "\tTrain Loss: 1.426 | Train Acc: 36.25%\n",
            "\t Val. Loss: 1.448 |  Val. Acc: 31.94%\n",
            "\tTrain Loss: 1.439 | Train Acc: 32.88%\n",
            "\t Val. Loss: 1.430 |  Val. Acc: 36.85%\n",
            "\tTrain Loss: 1.422 | Train Acc: 36.56%\n",
            "\t Val. Loss: 1.444 |  Val. Acc: 32.11%\n",
            "\tTrain Loss: 1.436 | Train Acc: 33.36%\n",
            "\t Val. Loss: 1.426 |  Val. Acc: 37.10%\n",
            "\tTrain Loss: 1.419 | Train Acc: 36.85%\n",
            "\t Val. Loss: 1.441 |  Val. Acc: 32.25%\n",
            "\tTrain Loss: 1.432 | Train Acc: 33.53%\n",
            "\t Val. Loss: 1.423 |  Val. Acc: 37.19%\n",
            "\tTrain Loss: 1.415 | Train Acc: 37.02%\n",
            "\t Val. Loss: 1.438 |  Val. Acc: 32.45%\n",
            "\tTrain Loss: 1.429 | Train Acc: 33.80%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 37.26%\n",
            "\tTrain Loss: 1.412 | Train Acc: 37.38%\n",
            "\t Val. Loss: 1.435 |  Val. Acc: 32.35%\n",
            "\tTrain Loss: 1.426 | Train Acc: 33.88%\n",
            "\t Val. Loss: 1.418 |  Val. Acc: 37.53%\n",
            "\tTrain Loss: 1.409 | Train Acc: 37.42%\n",
            "\t Val. Loss: 1.433 |  Val. Acc: 32.69%\n",
            "\tTrain Loss: 1.423 | Train Acc: 34.08%\n",
            "\t Val. Loss: 1.415 |  Val. Acc: 37.76%\n",
            "\tTrain Loss: 1.406 | Train Acc: 37.74%\n",
            "\t Val. Loss: 1.430 |  Val. Acc: 32.99%\n",
            "\tTrain Loss: 1.421 | Train Acc: 34.26%\n",
            "\t Val. Loss: 1.412 |  Val. Acc: 37.74%\n",
            "\tTrain Loss: 1.403 | Train Acc: 37.92%\n",
            "\t Val. Loss: 1.428 |  Val. Acc: 33.35%\n",
            "\tTrain Loss: 1.418 | Train Acc: 34.63%\n",
            "\t Val. Loss: 1.410 |  Val. Acc: 37.90%\n",
            "\tTrain Loss: 1.400 | Train Acc: 38.14%\n",
            "\t Val. Loss: 1.425 |  Val. Acc: 33.59%\n",
            "\tTrain Loss: 1.415 | Train Acc: 34.79%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 38.21%\n",
            "\tTrain Loss: 1.398 | Train Acc: 38.48%\n",
            "\t Val. Loss: 1.423 |  Val. Acc: 33.83%\n",
            "\tTrain Loss: 1.412 | Train Acc: 34.99%\n",
            "\t Val. Loss: 1.405 |  Val. Acc: 38.37%\n",
            "\tTrain Loss: 1.395 | Train Acc: 38.90%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 33.96%\n",
            "\tTrain Loss: 1.410 | Train Acc: 35.18%\n",
            "\t Val. Loss: 1.403 |  Val. Acc: 38.73%\n",
            "\tTrain Loss: 1.392 | Train Acc: 39.10%\n",
            "\t Val. Loss: 1.418 |  Val. Acc: 34.10%\n",
            "\tTrain Loss: 1.407 | Train Acc: 35.28%\n",
            "\t Val. Loss: 1.400 |  Val. Acc: 38.69%\n",
            "\tTrain Loss: 1.390 | Train Acc: 39.40%\n",
            "\t Val. Loss: 1.414 |  Val. Acc: 34.41%\n",
            "\tTrain Loss: 1.404 | Train Acc: 35.45%\n",
            "\t Val. Loss: 1.398 |  Val. Acc: 38.69%\n",
            "\tTrain Loss: 1.386 | Train Acc: 39.54%\n",
            "\t Val. Loss: 1.412 |  Val. Acc: 34.90%\n",
            "\tTrain Loss: 1.401 | Train Acc: 35.77%\n",
            "\t Val. Loss: 1.395 |  Val. Acc: 38.82%\n",
            "\tTrain Loss: 1.383 | Train Acc: 39.65%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 35.11%\n",
            "\tTrain Loss: 1.398 | Train Acc: 35.84%\n",
            "\t Val. Loss: 1.391 |  Val. Acc: 39.20%\n",
            "\tTrain Loss: 1.379 | Train Acc: 39.87%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 35.36%\n",
            "\tTrain Loss: 1.395 | Train Acc: 36.08%\n",
            "\t Val. Loss: 1.387 |  Val. Acc: 39.66%\n",
            "\tTrain Loss: 1.375 | Train Acc: 40.31%\n",
            "\t Val. Loss: 1.403 |  Val. Acc: 35.62%\n",
            "\tTrain Loss: 1.391 | Train Acc: 36.17%\n",
            "\t Val. Loss: 1.383 |  Val. Acc: 39.47%\n",
            "\tTrain Loss: 1.370 | Train Acc: 40.27%\n",
            "\t Val. Loss: 1.399 |  Val. Acc: 36.00%\n",
            "\tTrain Loss: 1.386 | Train Acc: 36.73%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 39.75%\n",
            "\tTrain Loss: 1.368 | Train Acc: 40.51%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 36.11%\n",
            "\tTrain Loss: 1.383 | Train Acc: 36.90%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 40.17%\n",
            "\tTrain Loss: 1.364 | Train Acc: 40.92%\n",
            "\t Val. Loss: 1.394 |  Val. Acc: 36.31%\n",
            "\tTrain Loss: 1.381 | Train Acc: 36.95%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 40.09%\n",
            "\tTrain Loss: 1.362 | Train Acc: 41.04%\n",
            "\t Val. Loss: 1.392 |  Val. Acc: 36.25%\n",
            "\tTrain Loss: 1.378 | Train Acc: 37.31%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 40.01%\n",
            "\tTrain Loss: 1.359 | Train Acc: 41.19%\n",
            "\t Val. Loss: 1.390 |  Val. Acc: 36.68%\n",
            "\tTrain Loss: 1.376 | Train Acc: 37.66%\n",
            "\t Val. Loss: 1.371 |  Val. Acc: 40.61%\n",
            "\tTrain Loss: 1.356 | Train Acc: 41.55%\n",
            "\t Val. Loss: 1.388 |  Val. Acc: 37.06%\n",
            "\tTrain Loss: 1.373 | Train Acc: 37.95%\n",
            "\t Val. Loss: 1.369 |  Val. Acc: 40.58%\n",
            "\tTrain Loss: 1.353 | Train Acc: 41.61%\n",
            "\t Val. Loss: 1.386 |  Val. Acc: 37.25%\n",
            "\tTrain Loss: 1.371 | Train Acc: 38.28%\n",
            "\t Val. Loss: 1.367 |  Val. Acc: 40.53%\n",
            "\tTrain Loss: 1.351 | Train Acc: 41.42%\n",
            "\t Val. Loss: 1.384 |  Val. Acc: 37.20%\n",
            "\tTrain Loss: 1.369 | Train Acc: 38.32%\n",
            "\t Val. Loss: 1.365 |  Val. Acc: 40.60%\n",
            "\tTrain Loss: 1.350 | Train Acc: 41.55%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 37.66%\n",
            "\tTrain Loss: 1.367 | Train Acc: 38.62%\n",
            "\t Val. Loss: 1.363 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.347 | Train Acc: 41.78%\n",
            "\t Val. Loss: 1.381 |  Val. Acc: 38.01%\n",
            "\tTrain Loss: 1.366 | Train Acc: 38.88%\n",
            "\t Val. Loss: 1.361 |  Val. Acc: 40.75%\n",
            "\tTrain Loss: 1.345 | Train Acc: 41.83%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 38.17%\n",
            "\tTrain Loss: 1.364 | Train Acc: 38.96%\n",
            "\t Val. Loss: 1.359 |  Val. Acc: 40.61%\n",
            "\tTrain Loss: 1.343 | Train Acc: 41.89%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 38.19%\n",
            "\tTrain Loss: 1.363 | Train Acc: 39.10%\n",
            "\t Val. Loss: 1.358 |  Val. Acc: 40.70%\n",
            "\tTrain Loss: 1.341 | Train Acc: 42.00%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 38.30%\n",
            "\tTrain Loss: 1.361 | Train Acc: 39.18%\n",
            "\t Val. Loss: 1.356 |  Val. Acc: 40.78%\n",
            "\tTrain Loss: 1.340 | Train Acc: 42.11%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 38.42%\n",
            "\tTrain Loss: 1.360 | Train Acc: 39.29%\n",
            "\t Val. Loss: 1.355 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.339 | Train Acc: 42.16%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 38.64%\n",
            "\tTrain Loss: 1.358 | Train Acc: 39.36%\n",
            "\t Val. Loss: 1.354 |  Val. Acc: 40.76%\n",
            "\tTrain Loss: 1.337 | Train Acc: 42.24%\n",
            "\t Val. Loss: 1.372 |  Val. Acc: 38.60%\n",
            "\tTrain Loss: 1.357 | Train Acc: 39.45%\n",
            "\t Val. Loss: 1.353 |  Val. Acc: 40.81%\n",
            "\tTrain Loss: 1.336 | Train Acc: 42.24%\n",
            "\t Val. Loss: 1.371 |  Val. Acc: 38.73%\n",
            "\tTrain Loss: 1.356 | Train Acc: 39.57%\n",
            "\t Val. Loss: 1.352 |  Val. Acc: 40.85%\n",
            "\tTrain Loss: 1.335 | Train Acc: 42.26%\n",
            "\t Val. Loss: 1.370 |  Val. Acc: 38.73%\n",
            "\tTrain Loss: 1.354 | Train Acc: 39.58%\n",
            "\t Val. Loss: 1.351 |  Val. Acc: 40.81%\n",
            "\tTrain Loss: 1.333 | Train Acc: 42.31%\n",
            "\t Val. Loss: 1.369 |  Val. Acc: 38.80%\n",
            "\tTrain Loss: 1.353 | Train Acc: 39.76%\n",
            "\t Val. Loss: 1.349 |  Val. Acc: 41.19%\n",
            "\tTrain Loss: 1.332 | Train Acc: 42.39%\n",
            "\t Val. Loss: 1.368 |  Val. Acc: 38.82%\n",
            "\tTrain Loss: 1.351 | Train Acc: 39.83%\n",
            "\t Val. Loss: 1.348 |  Val. Acc: 40.95%\n",
            "\tTrain Loss: 1.331 | Train Acc: 42.45%\n",
            "\t Val. Loss: 1.366 |  Val. Acc: 38.82%\n",
            "\tTrain Loss: 1.350 | Train Acc: 39.96%\n",
            "\t Val. Loss: 1.346 |  Val. Acc: 41.30%\n",
            "\tTrain Loss: 1.328 | Train Acc: 42.60%\n",
            "\t Val. Loss: 1.365 |  Val. Acc: 39.31%\n",
            "\tTrain Loss: 1.349 | Train Acc: 40.33%\n",
            "\t Val. Loss: 1.345 |  Val. Acc: 41.25%\n",
            "\tTrain Loss: 1.327 | Train Acc: 42.63%\n",
            "\t Val. Loss: 1.364 |  Val. Acc: 39.26%\n",
            "\tTrain Loss: 1.347 | Train Acc: 40.39%\n",
            "\t Val. Loss: 1.344 |  Val. Acc: 41.16%\n",
            "\tTrain Loss: 1.326 | Train Acc: 42.61%\n",
            "\t Val. Loss: 1.363 |  Val. Acc: 39.36%\n",
            "\tTrain Loss: 1.346 | Train Acc: 40.46%\n",
            "\t Val. Loss: 1.343 |  Val. Acc: 41.29%\n",
            "\tTrain Loss: 1.324 | Train Acc: 42.81%\n",
            "\t Val. Loss: 1.361 |  Val. Acc: 39.40%\n",
            "\tTrain Loss: 1.345 | Train Acc: 40.60%\n",
            "\t Val. Loss: 1.342 |  Val. Acc: 41.36%\n",
            "\tTrain Loss: 1.323 | Train Acc: 42.86%\n",
            "\t Val. Loss: 1.360 |  Val. Acc: 39.54%\n",
            "\tTrain Loss: 1.343 | Train Acc: 40.64%\n",
            "\t Val. Loss: 1.341 |  Val. Acc: 41.28%\n",
            "\tTrain Loss: 1.322 | Train Acc: 42.80%\n",
            "\t Val. Loss: 1.359 |  Val. Acc: 39.51%\n",
            "\tTrain Loss: 1.342 | Train Acc: 40.71%\n",
            "\t Val. Loss: 1.340 |  Val. Acc: 41.23%\n",
            "\tTrain Loss: 1.321 | Train Acc: 42.88%\n",
            "\t Val. Loss: 1.358 |  Val. Acc: 39.59%\n",
            "\tTrain Loss: 1.341 | Train Acc: 40.82%\n",
            "\t Val. Loss: 1.339 |  Val. Acc: 41.53%\n",
            "\tTrain Loss: 1.320 | Train Acc: 42.91%\n",
            "\t Val. Loss: 1.357 |  Val. Acc: 39.74%\n",
            "\tTrain Loss: 1.339 | Train Acc: 40.88%\n",
            "\t Val. Loss: 1.338 |  Val. Acc: 41.45%\n",
            "\tTrain Loss: 1.319 | Train Acc: 42.99%\n",
            "\t Val. Loss: 1.355 |  Val. Acc: 39.71%\n",
            "\tTrain Loss: 1.338 | Train Acc: 41.04%\n",
            "\t Val. Loss: 1.337 |  Val. Acc: 41.48%\n",
            "\tTrain Loss: 1.318 | Train Acc: 43.06%\n",
            "\t Val. Loss: 1.355 |  Val. Acc: 39.81%\n",
            "\tTrain Loss: 1.337 | Train Acc: 41.11%\n",
            "\t Val. Loss: 1.336 |  Val. Acc: 41.50%\n",
            "\tTrain Loss: 1.317 | Train Acc: 43.05%\n",
            "\t Val. Loss: 1.354 |  Val. Acc: 39.77%\n",
            "\tTrain Loss: 1.336 | Train Acc: 41.21%\n",
            "\t Val. Loss: 1.336 |  Val. Acc: 41.64%\n",
            "\tTrain Loss: 1.316 | Train Acc: 43.07%\n",
            "\t Val. Loss: 1.353 |  Val. Acc: 39.64%\n",
            "\tTrain Loss: 1.335 | Train Acc: 41.26%\n",
            "\t Val. Loss: 1.335 |  Val. Acc: 41.75%\n",
            "\tTrain Loss: 1.315 | Train Acc: 43.14%\n",
            "\t Val. Loss: 1.352 |  Val. Acc: 39.79%\n",
            "\tTrain Loss: 1.333 | Train Acc: 41.32%\n",
            "\t Val. Loss: 1.334 |  Val. Acc: 41.75%\n",
            "\tTrain Loss: 1.314 | Train Acc: 43.20%\n",
            "\t Val. Loss: 1.350 |  Val. Acc: 39.83%\n",
            "\tTrain Loss: 1.332 | Train Acc: 41.42%\n",
            "\t Val. Loss: 1.333 |  Val. Acc: 41.84%\n",
            "\tTrain Loss: 1.313 | Train Acc: 43.21%\n",
            "\t Val. Loss: 1.350 |  Val. Acc: 39.70%\n",
            "\tTrain Loss: 1.331 | Train Acc: 41.40%\n",
            "\t Val. Loss: 1.331 |  Val. Acc: 42.05%\n",
            "\tTrain Loss: 1.311 | Train Acc: 43.27%\n",
            "\t Val. Loss: 1.349 |  Val. Acc: 39.45%\n",
            "\tTrain Loss: 1.330 | Train Acc: 41.29%\n",
            "\t Val. Loss: 1.331 |  Val. Acc: 42.07%\n",
            "\tTrain Loss: 1.310 | Train Acc: 43.35%\n",
            "\t Val. Loss: 1.348 |  Val. Acc: 39.68%\n",
            "\tTrain Loss: 1.329 | Train Acc: 41.34%\n",
            "\t Val. Loss: 1.329 |  Val. Acc: 42.25%\n",
            "\tTrain Loss: 1.308 | Train Acc: 43.49%\n",
            "\t Val. Loss: 1.347 |  Val. Acc: 40.08%\n",
            "\tTrain Loss: 1.328 | Train Acc: 41.63%\n",
            "\t Val. Loss: 1.328 |  Val. Acc: 42.41%\n",
            "\tTrain Loss: 1.307 | Train Acc: 43.47%\n",
            "\t Val. Loss: 1.346 |  Val. Acc: 40.04%\n",
            "\tTrain Loss: 1.327 | Train Acc: 41.50%\n",
            "\t Val. Loss: 1.327 |  Val. Acc: 42.31%\n",
            "\tTrain Loss: 1.306 | Train Acc: 43.44%\n",
            "\t Val. Loss: 1.346 |  Val. Acc: 39.83%\n",
            "\tTrain Loss: 1.326 | Train Acc: 41.07%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 41.59%\n",
            "\tTrain Loss: 1.306 | Train Acc: 43.01%\n",
            "\t Val. Loss: 1.348 |  Val. Acc: 39.14%\n",
            "\tTrain Loss: 1.328 | Train Acc: 40.32%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 41.39%\n",
            "\tTrain Loss: 1.303 | Train Acc: 42.68%\n",
            "\t Val. Loss: 1.346 |  Val. Acc: 39.49%\n",
            "\tTrain Loss: 1.326 | Train Acc: 40.64%\n",
            "\t Val. Loss: 1.322 |  Val. Acc: 41.64%\n",
            "\tTrain Loss: 1.302 | Train Acc: 42.78%\n",
            "\t Val. Loss: 1.345 |  Val. Acc: 39.55%\n",
            "\tTrain Loss: 1.324 | Train Acc: 40.65%\n",
            "\t Val. Loss: 1.321 |  Val. Acc: 41.70%\n",
            "\tTrain Loss: 1.300 | Train Acc: 42.83%\n",
            "\t Val. Loss: 1.344 |  Val. Acc: 39.63%\n",
            "\tTrain Loss: 1.323 | Train Acc: 40.69%\n",
            "\t Val. Loss: 1.320 |  Val. Acc: 41.74%\n",
            "\tTrain Loss: 1.299 | Train Acc: 42.90%\n",
            "\t Val. Loss: 1.343 |  Val. Acc: 39.66%\n",
            "\tTrain Loss: 1.322 | Train Acc: 40.80%\n",
            "\t Val. Loss: 1.319 |  Val. Acc: 41.82%\n",
            "\tTrain Loss: 1.298 | Train Acc: 42.93%\n",
            "\t Val. Loss: 1.342 |  Val. Acc: 39.77%\n",
            "\tTrain Loss: 1.321 | Train Acc: 40.83%\n",
            "\t Val. Loss: 1.317 |  Val. Acc: 41.79%\n",
            "\tTrain Loss: 1.296 | Train Acc: 42.97%\n",
            "\t Val. Loss: 1.341 |  Val. Acc: 39.75%\n",
            "\tTrain Loss: 1.319 | Train Acc: 40.84%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.84%\n",
            "\tTrain Loss: 1.295 | Train Acc: 43.06%\n",
            "\t Val. Loss: 1.339 |  Val. Acc: 39.79%\n",
            "\tTrain Loss: 1.318 | Train Acc: 40.89%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.74%\n",
            "\tTrain Loss: 1.295 | Train Acc: 43.08%\n",
            "\t Val. Loss: 1.338 |  Val. Acc: 39.84%\n",
            "\tTrain Loss: 1.317 | Train Acc: 40.92%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 41.91%\n",
            "\tTrain Loss: 1.294 | Train Acc: 43.14%\n",
            "\t Val. Loss: 1.337 |  Val. Acc: 39.93%\n",
            "\tTrain Loss: 1.316 | Train Acc: 40.98%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 42.09%\n",
            "\tTrain Loss: 1.294 | Train Acc: 43.17%\n",
            "\t Val. Loss: 1.336 |  Val. Acc: 39.97%\n",
            "\tTrain Loss: 1.315 | Train Acc: 41.01%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 42.24%\n",
            "\tTrain Loss: 1.293 | Train Acc: 43.19%\n",
            "\t Val. Loss: 1.335 |  Val. Acc: 40.11%\n",
            "\tTrain Loss: 1.314 | Train Acc: 41.13%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 42.36%\n",
            "\tTrain Loss: 1.293 | Train Acc: 43.23%\n",
            "\t Val. Loss: 1.335 |  Val. Acc: 40.21%\n",
            "\tTrain Loss: 1.313 | Train Acc: 41.17%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 42.24%\n",
            "\tTrain Loss: 1.293 | Train Acc: 43.24%\n",
            "\t Val. Loss: 1.334 |  Val. Acc: 40.24%\n",
            "\tTrain Loss: 1.313 | Train Acc: 41.18%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 42.39%\n",
            "\tTrain Loss: 1.292 | Train Acc: 43.22%\n",
            "\t Val. Loss: 1.333 |  Val. Acc: 40.22%\n",
            "\tTrain Loss: 1.312 | Train Acc: 41.25%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 42.34%\n",
            "\tTrain Loss: 1.292 | Train Acc: 43.29%\n",
            "\t Val. Loss: 1.333 |  Val. Acc: 40.34%\n",
            "\tTrain Loss: 1.311 | Train Acc: 41.28%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 42.34%\n",
            "\tTrain Loss: 1.292 | Train Acc: 43.27%\n",
            "\t Val. Loss: 1.332 |  Val. Acc: 40.33%\n",
            "\tTrain Loss: 1.311 | Train Acc: 41.35%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 42.34%\n",
            "\tTrain Loss: 1.291 | Train Acc: 43.25%\n",
            "\t Val. Loss: 1.332 |  Val. Acc: 40.35%\n",
            "\tTrain Loss: 1.310 | Train Acc: 41.40%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 42.38%\n",
            "\tTrain Loss: 1.291 | Train Acc: 43.24%\n",
            "\t Val. Loss: 1.331 |  Val. Acc: 40.36%\n",
            "\tTrain Loss: 1.310 | Train Acc: 41.42%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 42.38%\n",
            "\tTrain Loss: 1.291 | Train Acc: 43.29%\n",
            "\t Val. Loss: 1.331 |  Val. Acc: 40.31%\n",
            "\tTrain Loss: 1.309 | Train Acc: 41.45%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.290 | Train Acc: 43.34%\n",
            "\t Val. Loss: 1.330 |  Val. Acc: 40.33%\n",
            "\tTrain Loss: 1.308 | Train Acc: 41.49%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 42.50%\n",
            "\tTrain Loss: 1.290 | Train Acc: 43.35%\n",
            "\t Val. Loss: 1.330 |  Val. Acc: 40.40%\n",
            "\tTrain Loss: 1.308 | Train Acc: 41.53%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 42.46%\n",
            "\tTrain Loss: 1.290 | Train Acc: 43.38%\n",
            "\t Val. Loss: 1.329 |  Val. Acc: 40.39%\n",
            "\tTrain Loss: 1.307 | Train Acc: 41.53%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 42.63%\n",
            "\tTrain Loss: 1.289 | Train Acc: 43.42%\n",
            "\t Val. Loss: 1.329 |  Val. Acc: 40.38%\n",
            "\tTrain Loss: 1.307 | Train Acc: 41.55%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 42.60%\n",
            "\tTrain Loss: 1.289 | Train Acc: 43.42%\n",
            "\t Val. Loss: 1.328 |  Val. Acc: 40.54%\n",
            "\tTrain Loss: 1.306 | Train Acc: 41.60%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 42.54%\n",
            "\tTrain Loss: 1.289 | Train Acc: 43.43%\n",
            "\t Val. Loss: 1.328 |  Val. Acc: 40.54%\n",
            "\tTrain Loss: 1.306 | Train Acc: 41.64%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 42.55%\n",
            "\tTrain Loss: 1.289 | Train Acc: 43.45%\n",
            "\t Val. Loss: 1.327 |  Val. Acc: 40.54%\n",
            "\tTrain Loss: 1.305 | Train Acc: 41.68%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 42.60%\n",
            "\tTrain Loss: 1.288 | Train Acc: 43.44%\n",
            "\t Val. Loss: 1.327 |  Val. Acc: 40.59%\n",
            "\tTrain Loss: 1.305 | Train Acc: 41.71%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 42.69%\n",
            "\tTrain Loss: 1.288 | Train Acc: 43.47%\n",
            "\t Val. Loss: 1.327 |  Val. Acc: 40.56%\n",
            "\tTrain Loss: 1.304 | Train Acc: 41.78%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 42.63%\n",
            "\tTrain Loss: 1.288 | Train Acc: 43.47%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 40.58%\n",
            "\tTrain Loss: 1.304 | Train Acc: 41.79%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 42.65%\n",
            "\tTrain Loss: 1.288 | Train Acc: 43.49%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 40.58%\n",
            "\tTrain Loss: 1.304 | Train Acc: 41.83%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 42.63%\n",
            "\tTrain Loss: 1.287 | Train Acc: 43.51%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 40.60%\n",
            "\tTrain Loss: 1.303 | Train Acc: 41.87%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 42.60%\n",
            "\tTrain Loss: 1.287 | Train Acc: 43.50%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 40.62%\n",
            "\tTrain Loss: 1.303 | Train Acc: 41.87%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 42.59%\n",
            "\tTrain Loss: 1.287 | Train Acc: 43.53%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 40.69%\n",
            "\tTrain Loss: 1.302 | Train Acc: 41.92%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 42.61%\n",
            "\tTrain Loss: 1.287 | Train Acc: 43.53%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.302 | Train Acc: 41.91%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 42.68%\n",
            "\tTrain Loss: 1.286 | Train Acc: 43.56%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 40.71%\n",
            "\tTrain Loss: 1.302 | Train Acc: 41.93%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 42.65%\n",
            "\tTrain Loss: 1.286 | Train Acc: 43.56%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.301 | Train Acc: 41.97%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.286 | Train Acc: 43.63%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 40.86%\n",
            "\tTrain Loss: 1.301 | Train Acc: 42.00%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.286 | Train Acc: 43.68%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 40.99%\n",
            "\tTrain Loss: 1.301 | Train Acc: 42.23%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 43.01%\n",
            "\tTrain Loss: 1.286 | Train Acc: 43.86%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 41.15%\n",
            "\tTrain Loss: 1.300 | Train Acc: 42.25%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 43.04%\n",
            "\tTrain Loss: 1.285 | Train Acc: 43.83%\n",
            "\t Val. Loss: 1.322 |  Val. Acc: 41.15%\n",
            "\tTrain Loss: 1.300 | Train Acc: 42.27%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 43.09%\n",
            "\tTrain Loss: 1.285 | Train Acc: 43.85%\n",
            "\t Val. Loss: 1.322 |  Val. Acc: 41.20%\n",
            "\tTrain Loss: 1.300 | Train Acc: 42.28%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 43.01%\n",
            "\tTrain Loss: 1.285 | Train Acc: 43.90%\n",
            "\t Val. Loss: 1.322 |  Val. Acc: 41.16%\n",
            "\tTrain Loss: 1.299 | Train Acc: 42.32%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 43.04%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.90%\n",
            "\t Val. Loss: 1.321 |  Val. Acc: 41.17%\n",
            "\tTrain Loss: 1.299 | Train Acc: 42.34%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 43.00%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.93%\n",
            "\t Val. Loss: 1.321 |  Val. Acc: 41.16%\n",
            "\tTrain Loss: 1.299 | Train Acc: 42.35%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 43.01%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.321 |  Val. Acc: 41.16%\n",
            "\tTrain Loss: 1.298 | Train Acc: 42.38%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 42.99%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.96%\n",
            "\t Val. Loss: 1.321 |  Val. Acc: 41.09%\n",
            "\tTrain Loss: 1.298 | Train Acc: 42.38%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 43.01%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.97%\n",
            "\t Val. Loss: 1.320 |  Val. Acc: 41.08%\n",
            "\tTrain Loss: 1.298 | Train Acc: 42.38%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 43.00%\n",
            "\tTrain Loss: 1.283 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.320 |  Val. Acc: 41.14%\n",
            "\tTrain Loss: 1.298 | Train Acc: 42.41%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 43.00%\n",
            "\tTrain Loss: 1.283 | Train Acc: 44.01%\n",
            "\t Val. Loss: 1.320 |  Val. Acc: 41.11%\n",
            "\tTrain Loss: 1.297 | Train Acc: 42.38%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 43.00%\n",
            "\tTrain Loss: 1.283 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.320 |  Val. Acc: 41.11%\n",
            "\tTrain Loss: 1.297 | Train Acc: 42.38%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 43.02%\n",
            "\tTrain Loss: 1.283 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.319 |  Val. Acc: 41.11%\n",
            "\tTrain Loss: 1.297 | Train Acc: 42.37%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 43.05%\n",
            "\tTrain Loss: 1.283 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.319 |  Val. Acc: 41.17%\n",
            "\tTrain Loss: 1.297 | Train Acc: 42.36%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 43.06%\n",
            "\tTrain Loss: 1.282 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.319 |  Val. Acc: 41.20%\n",
            "\tTrain Loss: 1.296 | Train Acc: 42.40%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 43.09%\n",
            "\tTrain Loss: 1.282 | Train Acc: 44.17%\n",
            "\t Val. Loss: 1.319 |  Val. Acc: 41.19%\n",
            "\tTrain Loss: 1.296 | Train Acc: 42.41%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 43.14%\n",
            "\tTrain Loss: 1.282 | Train Acc: 44.17%\n",
            "\t Val. Loss: 1.319 |  Val. Acc: 41.21%\n",
            "\tTrain Loss: 1.296 | Train Acc: 42.40%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 43.08%\n",
            "\tTrain Loss: 1.282 | Train Acc: 44.18%\n",
            "\t Val. Loss: 1.318 |  Val. Acc: 41.29%\n",
            "\tTrain Loss: 1.296 | Train Acc: 42.41%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 43.10%\n",
            "\tTrain Loss: 1.282 | Train Acc: 44.21%\n",
            "\t Val. Loss: 1.318 |  Val. Acc: 41.28%\n",
            "\tTrain Loss: 1.295 | Train Acc: 42.44%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 43.15%\n",
            "\tTrain Loss: 1.282 | Train Acc: 44.19%\n",
            "\t Val. Loss: 1.318 |  Val. Acc: 41.23%\n",
            "\tTrain Loss: 1.295 | Train Acc: 42.45%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 43.13%\n",
            "\tTrain Loss: 1.281 | Train Acc: 44.19%\n",
            "\t Val. Loss: 1.318 |  Val. Acc: 41.23%\n",
            "\tTrain Loss: 1.295 | Train Acc: 42.47%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 43.16%\n",
            "\tTrain Loss: 1.281 | Train Acc: 44.20%\n",
            "\t Val. Loss: 1.318 |  Val. Acc: 41.24%\n",
            "\tTrain Loss: 1.295 | Train Acc: 42.47%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 43.14%\n",
            "\tTrain Loss: 1.281 | Train Acc: 44.19%\n",
            "\t Val. Loss: 1.317 |  Val. Acc: 41.32%\n",
            "\tTrain Loss: 1.295 | Train Acc: 42.47%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 43.15%\n",
            "\tTrain Loss: 1.281 | Train Acc: 44.19%\n",
            "\t Val. Loss: 1.317 |  Val. Acc: 41.31%\n",
            "\tTrain Loss: 1.294 | Train Acc: 42.52%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 43.14%\n",
            "\tTrain Loss: 1.281 | Train Acc: 44.23%\n",
            "\t Val. Loss: 1.317 |  Val. Acc: 41.26%\n",
            "\tTrain Loss: 1.294 | Train Acc: 42.53%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 43.13%\n",
            "\tTrain Loss: 1.281 | Train Acc: 44.23%\n",
            "\t Val. Loss: 1.317 |  Val. Acc: 41.29%\n",
            "\tTrain Loss: 1.294 | Train Acc: 42.51%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 43.14%\n",
            "\tTrain Loss: 1.280 | Train Acc: 44.25%\n",
            "\t Val. Loss: 1.317 |  Val. Acc: 41.35%\n",
            "\tTrain Loss: 1.294 | Train Acc: 42.51%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 43.17%\n",
            "\tTrain Loss: 1.280 | Train Acc: 44.27%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.26%\n",
            "\tTrain Loss: 1.293 | Train Acc: 42.54%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 42.99%\n",
            "\tTrain Loss: 1.280 | Train Acc: 44.16%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.06%\n",
            "\tTrain Loss: 1.293 | Train Acc: 42.39%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 43.04%\n",
            "\tTrain Loss: 1.280 | Train Acc: 44.15%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.09%\n",
            "\tTrain Loss: 1.293 | Train Acc: 42.43%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 43.00%\n",
            "\tTrain Loss: 1.280 | Train Acc: 44.15%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.04%\n",
            "\tTrain Loss: 1.293 | Train Acc: 42.41%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 42.97%\n",
            "\tTrain Loss: 1.279 | Train Acc: 44.15%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.15%\n",
            "\tTrain Loss: 1.292 | Train Acc: 42.38%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.279 | Train Acc: 44.11%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.24%\n",
            "\tTrain Loss: 1.292 | Train Acc: 42.39%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.279 | Train Acc: 44.17%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 41.19%\n",
            "\tTrain Loss: 1.292 | Train Acc: 42.40%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.84%\n",
            "\tTrain Loss: 1.279 | Train Acc: 44.12%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 41.20%\n",
            "\tTrain Loss: 1.292 | Train Acc: 42.40%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.279 | Train Acc: 44.16%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 41.25%\n",
            "\tTrain Loss: 1.291 | Train Acc: 42.40%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.279 | Train Acc: 44.16%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 41.19%\n",
            "\tTrain Loss: 1.291 | Train Acc: 42.39%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.17%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 41.19%\n",
            "\tTrain Loss: 1.291 | Train Acc: 42.41%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.16%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 41.17%\n",
            "\tTrain Loss: 1.291 | Train Acc: 42.43%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.14%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 41.19%\n",
            "\tTrain Loss: 1.291 | Train Acc: 42.44%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.21%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 41.29%\n",
            "\tTrain Loss: 1.290 | Train Acc: 42.47%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.21%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 41.30%\n",
            "\tTrain Loss: 1.290 | Train Acc: 42.49%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.20%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 41.32%\n",
            "\tTrain Loss: 1.290 | Train Acc: 42.53%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.14%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 41.34%\n",
            "\tTrain Loss: 1.290 | Train Acc: 42.52%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.13%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 41.36%\n",
            "\tTrain Loss: 1.290 | Train Acc: 42.52%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.84%\n",
            "\tTrain Loss: 1.278 | Train Acc: 44.14%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 41.36%\n",
            "\tTrain Loss: 1.289 | Train Acc: 42.54%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.15%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 41.37%\n",
            "\tTrain Loss: 1.289 | Train Acc: 42.53%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.16%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 41.39%\n",
            "\tTrain Loss: 1.289 | Train Acc: 42.56%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.20%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 41.40%\n",
            "\tTrain Loss: 1.289 | Train Acc: 42.57%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.17%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 41.37%\n",
            "\tTrain Loss: 1.289 | Train Acc: 42.58%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.18%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 41.40%\n",
            "\tTrain Loss: 1.289 | Train Acc: 42.58%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.95%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.19%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 41.37%\n",
            "\tTrain Loss: 1.288 | Train Acc: 42.59%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.22%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 41.41%\n",
            "\tTrain Loss: 1.288 | Train Acc: 42.60%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.21%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 41.42%\n",
            "\tTrain Loss: 1.288 | Train Acc: 42.60%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.95%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.22%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 41.39%\n",
            "\tTrain Loss: 1.288 | Train Acc: 42.60%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.99%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.21%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 41.42%\n",
            "\tTrain Loss: 1.288 | Train Acc: 42.63%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.97%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.25%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 41.37%\n",
            "\tTrain Loss: 1.288 | Train Acc: 42.63%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.99%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.26%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 41.36%\n",
            "\tTrain Loss: 1.287 | Train Acc: 42.62%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.99%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.27%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 41.40%\n",
            "\tTrain Loss: 1.287 | Train Acc: 42.61%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.97%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.27%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 41.42%\n",
            "\tTrain Loss: 1.287 | Train Acc: 42.63%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.99%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.27%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 41.41%\n",
            "\tTrain Loss: 1.287 | Train Acc: 42.64%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.94%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.27%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 41.41%\n",
            "\tTrain Loss: 1.287 | Train Acc: 42.67%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.95%\n",
            "\tTrain Loss: 1.276 | Train Acc: 44.29%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 41.48%\n",
            "\tTrain Loss: 1.287 | Train Acc: 42.69%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.99%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.29%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 41.46%\n",
            "\tTrain Loss: 1.286 | Train Acc: 42.70%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.13%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.33%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 41.53%\n",
            "\tTrain Loss: 1.286 | Train Acc: 42.76%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.17%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.40%\n",
            "\t Val. Loss: 1.311 |  Val. Acc: 41.54%\n",
            "\tTrain Loss: 1.286 | Train Acc: 42.83%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.17%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.40%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 41.54%\n",
            "\tTrain Loss: 1.286 | Train Acc: 42.84%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.17%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.42%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 41.56%\n",
            "\tTrain Loss: 1.286 | Train Acc: 42.86%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.16%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.42%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 41.56%\n",
            "\tTrain Loss: 1.285 | Train Acc: 42.87%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.19%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.45%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 41.57%\n",
            "\tTrain Loss: 1.285 | Train Acc: 42.90%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.21%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.46%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 41.46%\n",
            "\tTrain Loss: 1.285 | Train Acc: 42.91%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.24%\n",
            "\tTrain Loss: 1.274 | Train Acc: 44.48%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 41.49%\n",
            "\tTrain Loss: 1.285 | Train Acc: 42.93%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.24%\n",
            "\tTrain Loss: 1.274 | Train Acc: 44.50%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 41.55%\n",
            "\tTrain Loss: 1.285 | Train Acc: 42.95%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.26%\n",
            "\tTrain Loss: 1.274 | Train Acc: 44.51%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 41.50%\n",
            "\tTrain Loss: 1.285 | Train Acc: 42.96%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.25%\n",
            "\tTrain Loss: 1.274 | Train Acc: 44.52%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 41.44%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.01%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.274 | Train Acc: 44.61%\n",
            "\t Val. Loss: 1.309 |  Val. Acc: 41.51%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.274 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 41.50%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.75%\n",
            "\tTrain Loss: 1.274 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 41.53%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.11%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.75%\n",
            "\tTrain Loss: 1.274 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 41.55%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.10%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.74%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.64%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 41.53%\n",
            "\tTrain Loss: 1.283 | Train Acc: 43.12%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.72%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 41.57%\n",
            "\tTrain Loss: 1.283 | Train Acc: 43.16%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 41.57%\n",
            "\tTrain Loss: 1.283 | Train Acc: 43.16%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.308 |  Val. Acc: 41.59%\n",
            "\tTrain Loss: 1.283 | Train Acc: 43.15%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 41.59%\n",
            "\tTrain Loss: 1.283 | Train Acc: 43.16%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.61%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 41.62%\n",
            "\tTrain Loss: 1.283 | Train Acc: 43.19%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.61%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 41.59%\n",
            "\tTrain Loss: 1.282 | Train Acc: 43.16%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.60%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 41.59%\n",
            "\tTrain Loss: 1.282 | Train Acc: 43.16%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.72%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 41.61%\n",
            "\tTrain Loss: 1.282 | Train Acc: 43.17%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.76%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 41.56%\n",
            "\tTrain Loss: 1.282 | Train Acc: 43.18%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.74%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 41.55%\n",
            "\tTrain Loss: 1.282 | Train Acc: 43.17%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 41.60%\n",
            "\tTrain Loss: 1.282 | Train Acc: 43.20%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.61%\n",
            "\tTrain Loss: 1.282 | Train Acc: 43.22%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.75%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.62%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.22%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.78%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.64%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.24%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.76%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.68%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.22%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 43.81%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.60%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.70%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.24%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.81%\n",
            "\tTrain Loss: 1.272 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.70%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.24%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.84%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.73%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.26%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.87%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.78%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.28%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.87%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.80%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.27%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.87%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.64%\n",
            "\t Val. Loss: 1.306 |  Val. Acc: 41.79%\n",
            "\tTrain Loss: 1.281 | Train Acc: 43.27%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.85%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.66%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.78%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.28%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.85%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.64%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.78%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.28%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.86%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.64%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.80%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.31%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.89%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.79%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.30%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.94%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.80%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.30%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.87%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.78%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.31%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.89%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.66%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.81%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.33%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.89%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.84%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.33%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.90%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.84%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.33%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.82%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.30%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 43.94%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.85%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.32%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.69%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.82%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.29%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.90%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.70%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.79%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.30%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.72%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.81%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.32%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.73%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.85%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.34%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.94%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.78%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.84%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.34%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.87%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.80%\n",
            "\t Val. Loss: 1.305 |  Val. Acc: 41.84%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.34%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.90%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.81%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 41.86%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.36%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.85%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.80%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 41.87%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.37%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.90%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.84%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 41.89%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.37%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 43.96%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.86%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 41.87%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.35%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.90%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.84%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 42.43%\n",
            "\tTrain Loss: 1.279 | Train Acc: 43.70%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.85%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.67%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.89%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.66%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.92%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.86%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.67%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.90%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.69%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.87%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.84%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 42.46%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.70%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.90%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.50%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.73%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.90%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.72%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.92%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.72%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.278 | Train Acc: 43.73%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.94%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.73%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.94%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.86%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.52%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.75%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.95%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.74%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.95%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.72%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.92%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.43%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.72%\n",
            "\t Val. Loss: 1.294 |  Val. Acc: 43.94%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.52%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.76%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.89%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.86%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.46%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.76%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.86%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.89%\n",
            "\t Val. Loss: 1.303 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.91%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.50%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.81%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.85%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.89%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.50%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.81%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.86%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.80%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.89%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.51%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.80%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.89%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.91%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.82%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.84%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.92%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.84%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.85%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.92%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.84%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.86%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.92%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.276 | Train Acc: 43.83%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.84%\n",
            "\tTrain Loss: 1.266 | Train Acc: 44.93%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.82%\n",
            "\t Val. Loss: 1.293 |  Val. Acc: 43.86%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.34%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.64%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.76%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.32%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.61%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.72%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.39%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.60%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.75%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.89%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.36%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.61%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.75%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.302 |  Val. Acc: 42.34%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.64%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.74%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.89%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.35%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.63%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.74%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.36%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.64%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.72%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.38%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.62%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.39%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.63%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.40%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.63%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.40%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.63%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.89%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.41%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.64%\n",
            "\t Val. Loss: 1.292 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.30%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.64%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.87%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.32%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.65%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.27%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.66%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.26%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.66%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.91%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.30%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.68%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.29%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.67%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.30%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.68%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.31%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.68%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.34%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.68%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.90%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.39%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.68%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.92%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.40%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.67%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.91%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.41%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.67%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.92%\n",
            "\t Val. Loss: 1.301 |  Val. Acc: 42.46%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.68%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.94%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.41%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.69%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.94%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.43%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.70%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.94%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.39%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.70%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.264 | Train Acc: 44.94%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.36%\n",
            "\tTrain Loss: 1.274 | Train Acc: 43.71%\n",
            "\t Val. Loss: 1.291 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.93%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.35%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.72%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.92%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.38%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.73%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.91%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.43%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.77%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.94%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.41%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.76%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.75%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.96%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.76%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.72%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.94%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.77%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.96%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.79%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.95%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.96%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.96%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.79%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.96%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.97%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.77%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.98%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.98%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.46%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.78%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.263 | Train Acc: 45.00%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.43%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.76%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.263 | Train Acc: 45.00%\n",
            "\t Val. Loss: 1.300 |  Val. Acc: 42.46%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.77%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.263 | Train Acc: 45.00%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.77%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.99%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.82%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.263 | Train Acc: 45.00%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.80%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.03%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.46%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.83%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.81%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.81%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.44%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.83%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.83%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.43%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.83%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.46%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.83%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.09%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.51%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.84%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.08%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.83%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.09%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.85%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.09%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.88%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.11%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.51%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.89%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.08%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.54%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.90%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.52%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.90%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.54%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.56%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.91%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.56%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.91%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.08%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.57%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.91%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.59%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.91%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.56%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.57%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.262 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.56%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.91%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.04%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.60%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.91%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.04%\n",
            "\t Val. Loss: 1.299 |  Val. Acc: 42.60%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.04%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.63%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.05%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.63%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.93%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.60%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.61%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.93%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.63%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.64%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.91%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.64%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.59%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.06%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.64%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.08%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.68%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.89%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.09%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.69%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.93%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.10%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.69%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.93%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.72%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.11%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.70%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.93%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.11%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.71%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.93%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.11%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.70%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.90%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.10%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.70%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.12%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.69%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.71%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.12%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.70%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.69%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.12%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.271 | Train Acc: 43.93%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.13%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.70%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.13%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.12%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.92%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.13%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.94%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.71%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.95%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.95%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.95%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.71%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.96%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.96%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.97%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.98%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.96%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.98%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.298 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.75%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.98%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.77%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.77%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.98%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.80%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.80%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.260 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.269 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.80%\n",
            "\tTrain Loss: 1.269 | Train Acc: 43.98%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.269 | Train Acc: 43.99%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.02%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.77%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.02%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.67%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.01%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.75%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.75%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.01%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.01%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.02%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.66%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.00%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.80%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.77%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.02%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.80%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.65%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 42.75%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.01%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.74%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.02%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.75%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.01%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.02%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.80%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.269 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.63%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.17%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.64%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.05%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.17%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.05%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.80%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.14%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.05%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.84%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.14%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.84%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.17%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.16%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.02%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.17%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.84%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.286 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.03%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.18%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.05%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.89%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.84%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.296 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.258 | Train Acc: 45.19%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.84%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.04%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.05%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.05%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.53%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.53%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.51%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.89%\n",
            "\tTrain Loss: 1.268 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.20%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.05%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.51%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.51%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.94%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.53%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.21%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.54%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.94%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.94%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.26%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.05%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.88%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.86%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.06%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.85%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.07%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.89%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.89%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.08%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.59%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.10%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.61%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.60%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.10%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.285 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.10%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.11%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.10%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.23%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.94%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.26%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.26%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.256 | Train Acc: 45.26%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.94%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.256 | Train Acc: 45.28%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.93%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.56%\n",
            "\tTrain Loss: 1.256 | Train Acc: 45.24%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.90%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.58%\n",
            "\tTrain Loss: 1.256 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.295 |  Val. Acc: 42.91%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.284 |  Val. Acc: 43.56%\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "torch.manual_seed(0)\n",
        "from torch.optim import SGD\n",
        "from numpy import vstack\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def accuracy(probs, target):\n",
        "  winners = probs.argmax(dim=1)+1  # getting the class with max probability and adding 1 since pytorch has indices 0 to n\n",
        "  target=target+1\n",
        "  corrects = (winners == target)\n",
        "  accuracy = corrects.sum().float() / float(target.size(0))\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def train_model(train_dl, y_tr_simple, model):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  criterion = CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "  optimizer.zero_grad()\n",
        "  output = model(train_dl)\n",
        "  target=torch.from_numpy(y_tr_simple)\n",
        "  target=target-1\n",
        "  loss = criterion(output, target)\n",
        "  acc = accuracy(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  epoch_loss += loss.item()\n",
        "  epoch_acc += acc.item()\n",
        "  return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate_model(test_dl, y_ts, model):\n",
        "   epoch_loss = 0\n",
        "   epoch_acc = 0\n",
        "   criterion = CrossEntropyLoss()\n",
        "   with torch.no_grad():\n",
        "        yhat = model(test_dl)\n",
        "        target = torch.from_numpy(y_ts)\n",
        "        target=target-1\n",
        "        loss = criterion(yhat, target)\n",
        "        acc = accuracy(yhat, target)   \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        return epoch_loss, epoch_acc \n",
        "\n",
        "\n",
        "avgMLP_model = avgMLP(300)\n",
        "X_train = torch.from_numpy(X_tr_simple_avg)\n",
        "X_val = torch.from_numpy(X_val_simple_avg)\n",
        "X_test = torch.from_numpy(X_ts_simple_avg)\n",
        "X_train = X_train.float()\n",
        "X_val=X_val.float()\n",
        "X_test=X_test.float()\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(1000):\n",
        "   train_loss, train_acc = train_model(X_train, y_tr_simple_avg, avgMLP_model)\n",
        "   valid_loss, valid_acc = evaluate_model(X_val, y_val_simple_avg, avgMLP_model)\n",
        "   if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(avgMLP_model.state_dict(), 'saved_weights'+'_'+'avgMLP_model'+'.pt')\n",
        "   print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "   print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VL2xNS-AiN8",
        "outputId": "4643585a-6265-416d-cffd-9947d8e51a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " For MLP Feedforwrd taking avg of vectors: Test Acc: 44.80%\n"
          ]
        }
      ],
      "source": [
        "avgMLP_model.load_state_dict(torch.load(os.path.join(\"saved_weights_avgMLP_model.pt\")))\n",
        "test_loss, test_acc = evaluate_model(X_test, y_ts_simple_avg, avgMLP_model)\n",
        "print(f' For MLP Feedforwrd taking avg of vectors: Test Acc: {test_acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMxxWff7BnDl"
      },
      "source": [
        "**Feedforward Neural Networks  by concatenating the first 10 word vectors for each review**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBo7uXCuBmp9"
      },
      "outputs": [],
      "source": [
        "x_tr=[]\n",
        "y_tr=[]\n",
        "x_ts=[]\n",
        "y_ts=[]\n",
        "for i in range(len(train_data)):\n",
        "  count=0\n",
        "  doc=[]\n",
        "  for word in vars(train_data[i])['review_body']:\n",
        "    if count>=10:\n",
        "      break\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "    count+=1\n",
        "  for k in range(count,10):\n",
        "    doc.append(np.zeros(300))\n",
        "  x_tr.append(doc)\n",
        "  y_tr.append(vars(train_data[i])['star_rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM8dLg4AE0tj"
      },
      "outputs": [],
      "source": [
        "x_val=[]\n",
        "y_val=[]\n",
        "for i in range(len(validation_data)):\n",
        "  count=0\n",
        "  doc=[]\n",
        "  for word in vars(validation_data[i])['review_body']:\n",
        "    if count>=10:\n",
        "      break\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "    count+=1\n",
        "  for k in range(count,10):\n",
        "    doc.append(np.zeros(300))\n",
        "  x_val.append(doc)\n",
        "  y_val.append(vars(validation_data[i])['star_rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38SaYN5TI5DH"
      },
      "outputs": [],
      "source": [
        "for i in range(len(testing_data)):\n",
        "  count=0\n",
        "  doc=[]\n",
        "  for word in vars(testing_data[i])['review_body']:\n",
        "    if count>=10:\n",
        "      break\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "    count+=1\n",
        "  for k in range(count,10):\n",
        "    doc.append(np.zeros(300))\n",
        "  x_ts.append(doc)\n",
        "  y_ts.append(vars(testing_data[i])['star_rating'])\n",
        "\n",
        "X_tr_simple_concat = np.array(x_tr)\n",
        "X_val_simple_concat = np.array(x_val)\n",
        "X_ts_simple_concat = np.array(x_ts)\n",
        "y_tr_simple_concat = np.array(y_tr)\n",
        "y_val_simple_concat = np.array(y_val)\n",
        "y_ts_simple_concat = np.array(y_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_e3pIeah4_z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.nn import Sigmoid\n",
        "from numpy import vstack\n",
        "class concatMLP(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(concatMLP, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "        self.hidden_size1=50\n",
        "        self.hidden_size2=10\n",
        "        self.output_dim=5\n",
        "        self.hidden1 = torch.nn.Linear(input_size, self.hidden_size1)\n",
        "        #kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        # second hidden layer\n",
        "        self.hidden2 = torch.nn.Linear(self.hidden_size1, self.hidden_size2)\n",
        "        #kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = torch.nn.ReLU()\n",
        "        # third hidden layer and output\n",
        "        self.hidden3 = torch.nn.Linear(self.hidden_size2, self.output_dim)\n",
        "        #xavier_uniform_(self.hidden3.weight)\n",
        "        self.act3 = Sigmoid()\n",
        " \n",
        "        \n",
        "    def forward(self, X):\n",
        "        X = X.view(X.size(0), -1) \n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "        #X = self.dropout(X)\n",
        "         # second hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        #X = self.dropout(X)\n",
        "        # third hidden layer and output\n",
        "        X = self.hidden3(X)\n",
        "        #X = self.act3(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJpcdPhZk-Lw",
        "outputId": "20f72faf-48e5-40a1-c58e-d5402129afc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.618 | Train Acc: 19.98%\n",
            "\t Val. Loss: 1.609 |  Val. Acc: 20.64%\n",
            "\tTrain Loss: 1.607 | Train Acc: 21.44%\n",
            "\t Val. Loss: 1.592 |  Val. Acc: 21.96%\n",
            "\tTrain Loss: 1.591 | Train Acc: 22.25%\n",
            "\t Val. Loss: 1.569 |  Val. Acc: 25.92%\n",
            "\tTrain Loss: 1.566 | Train Acc: 27.00%\n",
            "\t Val. Loss: 1.582 |  Val. Acc: 30.06%\n",
            "\tTrain Loss: 1.582 | Train Acc: 30.46%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 31.05%\n",
            "\tTrain Loss: 1.546 | Train Acc: 31.46%\n",
            "\t Val. Loss: 1.568 |  Val. Acc: 28.59%\n",
            "\tTrain Loss: 1.565 | Train Acc: 28.84%\n",
            "\t Val. Loss: 1.522 |  Val. Acc: 32.51%\n",
            "\tTrain Loss: 1.517 | Train Acc: 33.06%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 30.02%\n",
            "\tTrain Loss: 1.533 | Train Acc: 30.43%\n",
            "\t Val. Loss: 1.501 |  Val. Acc: 33.68%\n",
            "\tTrain Loss: 1.493 | Train Acc: 34.75%\n",
            "\t Val. Loss: 1.515 |  Val. Acc: 32.37%\n",
            "\tTrain Loss: 1.509 | Train Acc: 32.83%\n",
            "\t Val. Loss: 1.481 |  Val. Acc: 34.83%\n",
            "\tTrain Loss: 1.470 | Train Acc: 36.23%\n",
            "\t Val. Loss: 1.498 |  Val. Acc: 32.91%\n",
            "\tTrain Loss: 1.489 | Train Acc: 33.93%\n",
            "\t Val. Loss: 1.468 |  Val. Acc: 35.94%\n",
            "\tTrain Loss: 1.454 | Train Acc: 36.85%\n",
            "\t Val. Loss: 1.481 |  Val. Acc: 34.09%\n",
            "\tTrain Loss: 1.470 | Train Acc: 35.31%\n",
            "\t Val. Loss: 1.458 |  Val. Acc: 36.34%\n",
            "\tTrain Loss: 1.441 | Train Acc: 37.66%\n",
            "\t Val. Loss: 1.470 |  Val. Acc: 34.50%\n",
            "\tTrain Loss: 1.457 | Train Acc: 36.18%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 36.59%\n",
            "\tTrain Loss: 1.433 | Train Acc: 37.91%\n",
            "\t Val. Loss: 1.487 |  Val. Acc: 33.89%\n",
            "\tTrain Loss: 1.473 | Train Acc: 34.83%\n",
            "\t Val. Loss: 1.431 |  Val. Acc: 37.34%\n",
            "\tTrain Loss: 1.414 | Train Acc: 38.53%\n",
            "\t Val. Loss: 1.461 |  Val. Acc: 34.15%\n",
            "\tTrain Loss: 1.445 | Train Acc: 35.02%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 38.20%\n",
            "\tTrain Loss: 1.390 | Train Acc: 39.06%\n",
            "\t Val. Loss: 1.446 |  Val. Acc: 34.61%\n",
            "\tTrain Loss: 1.428 | Train Acc: 35.88%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 38.93%\n",
            "\tTrain Loss: 1.374 | Train Acc: 39.70%\n",
            "\t Val. Loss: 1.439 |  Val. Acc: 35.41%\n",
            "\tTrain Loss: 1.419 | Train Acc: 36.57%\n",
            "\t Val. Loss: 1.394 |  Val. Acc: 38.93%\n",
            "\tTrain Loss: 1.370 | Train Acc: 40.38%\n",
            "\t Val. Loss: 1.433 |  Val. Acc: 35.95%\n",
            "\tTrain Loss: 1.410 | Train Acc: 37.41%\n",
            "\t Val. Loss: 1.394 |  Val. Acc: 39.32%\n",
            "\tTrain Loss: 1.369 | Train Acc: 40.84%\n",
            "\t Val. Loss: 1.435 |  Val. Acc: 36.30%\n",
            "\tTrain Loss: 1.411 | Train Acc: 38.09%\n",
            "\t Val. Loss: 1.394 |  Val. Acc: 39.01%\n",
            "\tTrain Loss: 1.367 | Train Acc: 40.87%\n",
            "\t Val. Loss: 1.426 |  Val. Acc: 37.66%\n",
            "\tTrain Loss: 1.399 | Train Acc: 38.98%\n",
            "\t Val. Loss: 1.391 |  Val. Acc: 38.76%\n",
            "\tTrain Loss: 1.361 | Train Acc: 40.85%\n",
            "\t Val. Loss: 1.418 |  Val. Acc: 37.91%\n",
            "\tTrain Loss: 1.388 | Train Acc: 39.64%\n",
            "\t Val. Loss: 1.385 |  Val. Acc: 38.00%\n",
            "\tTrain Loss: 1.355 | Train Acc: 40.34%\n",
            "\t Val. Loss: 1.418 |  Val. Acc: 38.59%\n",
            "\tTrain Loss: 1.384 | Train Acc: 39.83%\n",
            "\t Val. Loss: 1.386 |  Val. Acc: 36.04%\n",
            "\tTrain Loss: 1.356 | Train Acc: 38.72%\n",
            "\t Val. Loss: 1.393 |  Val. Acc: 38.80%\n",
            "\tTrain Loss: 1.361 | Train Acc: 40.10%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 36.60%\n",
            "\tTrain Loss: 1.345 | Train Acc: 39.12%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 39.19%\n",
            "\tTrain Loss: 1.347 | Train Acc: 40.70%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 36.38%\n",
            "\tTrain Loss: 1.341 | Train Acc: 39.09%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 39.38%\n",
            "\tTrain Loss: 1.338 | Train Acc: 41.06%\n",
            "\t Val. Loss: 1.367 |  Val. Acc: 36.46%\n",
            "\tTrain Loss: 1.334 | Train Acc: 39.40%\n",
            "\t Val. Loss: 1.367 |  Val. Acc: 39.72%\n",
            "\tTrain Loss: 1.330 | Train Acc: 41.28%\n",
            "\t Val. Loss: 1.363 |  Val. Acc: 36.95%\n",
            "\tTrain Loss: 1.328 | Train Acc: 39.47%\n",
            "\t Val. Loss: 1.361 |  Val. Acc: 40.06%\n",
            "\tTrain Loss: 1.322 | Train Acc: 41.52%\n",
            "\t Val. Loss: 1.364 |  Val. Acc: 36.96%\n",
            "\tTrain Loss: 1.326 | Train Acc: 39.43%\n",
            "\t Val. Loss: 1.358 |  Val. Acc: 39.74%\n",
            "\tTrain Loss: 1.317 | Train Acc: 41.63%\n",
            "\t Val. Loss: 1.393 |  Val. Acc: 37.95%\n",
            "\tTrain Loss: 1.356 | Train Acc: 40.51%\n",
            "\t Val. Loss: 1.359 |  Val. Acc: 39.96%\n",
            "\tTrain Loss: 1.317 | Train Acc: 42.41%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 37.87%\n",
            "\tTrain Loss: 1.381 | Train Acc: 40.18%\n",
            "\t Val. Loss: 1.353 |  Val. Acc: 40.95%\n",
            "\tTrain Loss: 1.310 | Train Acc: 43.10%\n",
            "\t Val. Loss: 1.417 |  Val. Acc: 37.87%\n",
            "\tTrain Loss: 1.377 | Train Acc: 40.45%\n",
            "\t Val. Loss: 1.352 |  Val. Acc: 40.83%\n",
            "\tTrain Loss: 1.305 | Train Acc: 43.17%\n",
            "\t Val. Loss: 1.415 |  Val. Acc: 38.25%\n",
            "\tTrain Loss: 1.373 | Train Acc: 40.50%\n",
            "\t Val. Loss: 1.350 |  Val. Acc: 40.87%\n",
            "\tTrain Loss: 1.302 | Train Acc: 43.35%\n",
            "\t Val. Loss: 1.413 |  Val. Acc: 38.25%\n",
            "\tTrain Loss: 1.370 | Train Acc: 40.71%\n",
            "\t Val. Loss: 1.348 |  Val. Acc: 41.05%\n",
            "\tTrain Loss: 1.299 | Train Acc: 43.44%\n",
            "\t Val. Loss: 1.410 |  Val. Acc: 38.51%\n",
            "\tTrain Loss: 1.366 | Train Acc: 40.97%\n",
            "\t Val. Loss: 1.347 |  Val. Acc: 40.96%\n",
            "\tTrain Loss: 1.296 | Train Acc: 43.68%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 38.53%\n",
            "\tTrain Loss: 1.362 | Train Acc: 41.10%\n",
            "\t Val. Loss: 1.345 |  Val. Acc: 41.29%\n",
            "\tTrain Loss: 1.294 | Train Acc: 43.79%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 38.57%\n",
            "\tTrain Loss: 1.359 | Train Acc: 41.16%\n",
            "\t Val. Loss: 1.344 |  Val. Acc: 41.16%\n",
            "\tTrain Loss: 1.291 | Train Acc: 43.95%\n",
            "\t Val. Loss: 1.404 |  Val. Acc: 38.74%\n",
            "\tTrain Loss: 1.356 | Train Acc: 41.35%\n",
            "\t Val. Loss: 1.343 |  Val. Acc: 41.20%\n",
            "\tTrain Loss: 1.289 | Train Acc: 44.17%\n",
            "\t Val. Loss: 1.402 |  Val. Acc: 38.65%\n",
            "\tTrain Loss: 1.353 | Train Acc: 41.51%\n",
            "\t Val. Loss: 1.342 |  Val. Acc: 41.24%\n",
            "\tTrain Loss: 1.287 | Train Acc: 44.19%\n",
            "\t Val. Loss: 1.400 |  Val. Acc: 38.51%\n",
            "\tTrain Loss: 1.350 | Train Acc: 41.66%\n",
            "\t Val. Loss: 1.341 |  Val. Acc: 41.23%\n",
            "\tTrain Loss: 1.285 | Train Acc: 44.30%\n",
            "\t Val. Loss: 1.399 |  Val. Acc: 38.55%\n",
            "\tTrain Loss: 1.348 | Train Acc: 41.81%\n",
            "\t Val. Loss: 1.340 |  Val. Acc: 41.40%\n",
            "\tTrain Loss: 1.283 | Train Acc: 44.44%\n",
            "\t Val. Loss: 1.397 |  Val. Acc: 38.76%\n",
            "\tTrain Loss: 1.345 | Train Acc: 42.23%\n",
            "\t Val. Loss: 1.339 |  Val. Acc: 41.40%\n",
            "\tTrain Loss: 1.281 | Train Acc: 44.49%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 38.75%\n",
            "\tTrain Loss: 1.343 | Train Acc: 42.07%\n",
            "\t Val. Loss: 1.339 |  Val. Acc: 41.28%\n",
            "\tTrain Loss: 1.280 | Train Acc: 44.78%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 39.05%\n",
            "\tTrain Loss: 1.342 | Train Acc: 42.01%\n",
            "\t Val. Loss: 1.338 |  Val. Acc: 41.10%\n",
            "\tTrain Loss: 1.279 | Train Acc: 44.48%\n",
            "\t Val. Loss: 1.402 |  Val. Acc: 38.89%\n",
            "\tTrain Loss: 1.344 | Train Acc: 42.24%\n",
            "\t Val. Loss: 1.336 |  Val. Acc: 41.06%\n",
            "\tTrain Loss: 1.279 | Train Acc: 44.51%\n",
            "\t Val. Loss: 1.399 |  Val. Acc: 39.15%\n",
            "\tTrain Loss: 1.340 | Train Acc: 42.35%\n",
            "\t Val. Loss: 1.336 |  Val. Acc: 41.14%\n",
            "\tTrain Loss: 1.277 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.397 |  Val. Acc: 39.16%\n",
            "\tTrain Loss: 1.336 | Train Acc: 42.44%\n",
            "\t Val. Loss: 1.335 |  Val. Acc: 40.98%\n",
            "\tTrain Loss: 1.275 | Train Acc: 44.77%\n",
            "\t Val. Loss: 1.394 |  Val. Acc: 39.49%\n",
            "\tTrain Loss: 1.333 | Train Acc: 42.46%\n",
            "\t Val. Loss: 1.335 |  Val. Acc: 41.40%\n",
            "\tTrain Loss: 1.273 | Train Acc: 44.96%\n",
            "\t Val. Loss: 1.393 |  Val. Acc: 39.39%\n",
            "\tTrain Loss: 1.330 | Train Acc: 42.61%\n",
            "\t Val. Loss: 1.334 |  Val. Acc: 41.16%\n",
            "\tTrain Loss: 1.271 | Train Acc: 44.81%\n",
            "\t Val. Loss: 1.390 |  Val. Acc: 39.72%\n",
            "\tTrain Loss: 1.326 | Train Acc: 42.68%\n",
            "\t Val. Loss: 1.334 |  Val. Acc: 41.61%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.80%\n",
            "\t Val. Loss: 1.389 |  Val. Acc: 39.41%\n",
            "\tTrain Loss: 1.323 | Train Acc: 42.74%\n",
            "\t Val. Loss: 1.334 |  Val. Acc: 41.36%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.74%\n",
            "\t Val. Loss: 1.387 |  Val. Acc: 39.74%\n",
            "\tTrain Loss: 1.320 | Train Acc: 42.81%\n",
            "\t Val. Loss: 1.333 |  Val. Acc: 41.64%\n",
            "\tTrain Loss: 1.265 | Train Acc: 44.96%\n",
            "\t Val. Loss: 1.386 |  Val. Acc: 39.49%\n",
            "\tTrain Loss: 1.317 | Train Acc: 42.96%\n",
            "\t Val. Loss: 1.332 |  Val. Acc: 41.57%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.97%\n",
            "\t Val. Loss: 1.384 |  Val. Acc: 39.75%\n",
            "\tTrain Loss: 1.314 | Train Acc: 42.93%\n",
            "\t Val. Loss: 1.332 |  Val. Acc: 41.71%\n",
            "\tTrain Loss: 1.261 | Train Acc: 45.15%\n",
            "\t Val. Loss: 1.383 |  Val. Acc: 39.57%\n",
            "\tTrain Loss: 1.311 | Train Acc: 42.98%\n",
            "\t Val. Loss: 1.331 |  Val. Acc: 41.82%\n",
            "\tTrain Loss: 1.259 | Train Acc: 45.27%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 39.41%\n",
            "\tTrain Loss: 1.308 | Train Acc: 42.94%\n",
            "\t Val. Loss: 1.330 |  Val. Acc: 41.90%\n",
            "\tTrain Loss: 1.256 | Train Acc: 45.32%\n",
            "\t Val. Loss: 1.381 |  Val. Acc: 39.44%\n",
            "\tTrain Loss: 1.305 | Train Acc: 43.16%\n",
            "\t Val. Loss: 1.329 |  Val. Acc: 41.71%\n",
            "\tTrain Loss: 1.254 | Train Acc: 45.42%\n",
            "\t Val. Loss: 1.380 |  Val. Acc: 39.30%\n",
            "\tTrain Loss: 1.303 | Train Acc: 43.29%\n",
            "\t Val. Loss: 1.328 |  Val. Acc: 41.90%\n",
            "\tTrain Loss: 1.252 | Train Acc: 45.44%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 39.36%\n",
            "\tTrain Loss: 1.300 | Train Acc: 43.42%\n",
            "\t Val. Loss: 1.327 |  Val. Acc: 41.85%\n",
            "\tTrain Loss: 1.249 | Train Acc: 45.58%\n",
            "\t Val. Loss: 1.377 |  Val. Acc: 39.36%\n",
            "\tTrain Loss: 1.297 | Train Acc: 43.42%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 42.01%\n",
            "\tTrain Loss: 1.247 | Train Acc: 45.66%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 39.35%\n",
            "\tTrain Loss: 1.294 | Train Acc: 43.59%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 41.95%\n",
            "\tTrain Loss: 1.245 | Train Acc: 45.70%\n",
            "\t Val. Loss: 1.384 |  Val. Acc: 38.19%\n",
            "\tTrain Loss: 1.303 | Train Acc: 42.62%\n",
            "\t Val. Loss: 1.334 |  Val. Acc: 41.61%\n",
            "\tTrain Loss: 1.251 | Train Acc: 45.43%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 36.10%\n",
            "\tTrain Loss: 1.334 | Train Acc: 40.01%\n",
            "\t Val. Loss: 1.331 |  Val. Acc: 41.86%\n",
            "\tTrain Loss: 1.249 | Train Acc: 45.54%\n",
            "\t Val. Loss: 1.411 |  Val. Acc: 36.35%\n",
            "\tTrain Loss: 1.328 | Train Acc: 40.37%\n",
            "\t Val. Loss: 1.329 |  Val. Acc: 42.16%\n",
            "\tTrain Loss: 1.246 | Train Acc: 45.61%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 36.47%\n",
            "\tTrain Loss: 1.324 | Train Acc: 40.64%\n",
            "\t Val. Loss: 1.327 |  Val. Acc: 42.26%\n",
            "\tTrain Loss: 1.243 | Train Acc: 45.77%\n",
            "\t Val. Loss: 1.407 |  Val. Acc: 36.66%\n",
            "\tTrain Loss: 1.320 | Train Acc: 40.98%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 42.34%\n",
            "\tTrain Loss: 1.240 | Train Acc: 45.86%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 36.83%\n",
            "\tTrain Loss: 1.317 | Train Acc: 41.14%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.238 | Train Acc: 46.08%\n",
            "\t Val. Loss: 1.404 |  Val. Acc: 37.01%\n",
            "\tTrain Loss: 1.313 | Train Acc: 41.33%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.61%\n",
            "\tTrain Loss: 1.235 | Train Acc: 46.19%\n",
            "\t Val. Loss: 1.403 |  Val. Acc: 37.04%\n",
            "\tTrain Loss: 1.310 | Train Acc: 41.49%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.64%\n",
            "\tTrain Loss: 1.233 | Train Acc: 46.33%\n",
            "\t Val. Loss: 1.401 |  Val. Acc: 37.15%\n",
            "\tTrain Loss: 1.308 | Train Acc: 41.73%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.66%\n",
            "\tTrain Loss: 1.231 | Train Acc: 46.40%\n",
            "\t Val. Loss: 1.400 |  Val. Acc: 37.19%\n",
            "\tTrain Loss: 1.305 | Train Acc: 41.89%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.66%\n",
            "\tTrain Loss: 1.229 | Train Acc: 46.48%\n",
            "\t Val. Loss: 1.399 |  Val. Acc: 37.28%\n",
            "\tTrain Loss: 1.303 | Train Acc: 42.01%\n",
            "\t Val. Loss: 1.322 |  Val. Acc: 42.57%\n",
            "\tTrain Loss: 1.227 | Train Acc: 46.59%\n",
            "\t Val. Loss: 1.398 |  Val. Acc: 37.39%\n",
            "\tTrain Loss: 1.300 | Train Acc: 42.16%\n",
            "\t Val. Loss: 1.321 |  Val. Acc: 42.65%\n",
            "\tTrain Loss: 1.225 | Train Acc: 46.64%\n",
            "\t Val. Loss: 1.397 |  Val. Acc: 37.50%\n",
            "\tTrain Loss: 1.298 | Train Acc: 42.31%\n",
            "\t Val. Loss: 1.321 |  Val. Acc: 42.68%\n",
            "\tTrain Loss: 1.223 | Train Acc: 46.66%\n",
            "\t Val. Loss: 1.397 |  Val. Acc: 37.41%\n",
            "\tTrain Loss: 1.296 | Train Acc: 42.48%\n",
            "\t Val. Loss: 1.320 |  Val. Acc: 42.56%\n",
            "\tTrain Loss: 1.221 | Train Acc: 46.70%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 37.50%\n",
            "\tTrain Loss: 1.293 | Train Acc: 42.58%\n",
            "\t Val. Loss: 1.319 |  Val. Acc: 42.63%\n",
            "\tTrain Loss: 1.219 | Train Acc: 46.74%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 37.45%\n",
            "\tTrain Loss: 1.292 | Train Acc: 42.72%\n",
            "\t Val. Loss: 1.318 |  Val. Acc: 42.40%\n",
            "\tTrain Loss: 1.217 | Train Acc: 46.73%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 37.64%\n",
            "\tTrain Loss: 1.289 | Train Acc: 42.73%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 42.32%\n",
            "\tTrain Loss: 1.215 | Train Acc: 46.86%\n",
            "\t Val. Loss: 1.395 |  Val. Acc: 37.64%\n",
            "\tTrain Loss: 1.287 | Train Acc: 42.82%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 42.43%\n",
            "\tTrain Loss: 1.213 | Train Acc: 46.97%\n",
            "\t Val. Loss: 1.394 |  Val. Acc: 37.67%\n",
            "\tTrain Loss: 1.284 | Train Acc: 43.01%\n",
            "\t Val. Loss: 1.316 |  Val. Acc: 42.52%\n",
            "\tTrain Loss: 1.211 | Train Acc: 47.02%\n",
            "\t Val. Loss: 1.393 |  Val. Acc: 37.80%\n",
            "\tTrain Loss: 1.282 | Train Acc: 43.05%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 42.63%\n",
            "\tTrain Loss: 1.209 | Train Acc: 47.16%\n",
            "\t Val. Loss: 1.392 |  Val. Acc: 37.80%\n",
            "\tTrain Loss: 1.280 | Train Acc: 43.13%\n",
            "\t Val. Loss: 1.314 |  Val. Acc: 42.56%\n",
            "\tTrain Loss: 1.207 | Train Acc: 47.17%\n",
            "\t Val. Loss: 1.392 |  Val. Acc: 37.86%\n",
            "\tTrain Loss: 1.277 | Train Acc: 43.30%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 42.59%\n",
            "\tTrain Loss: 1.205 | Train Acc: 47.33%\n",
            "\t Val. Loss: 1.391 |  Val. Acc: 37.94%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.44%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 42.57%\n",
            "\tTrain Loss: 1.203 | Train Acc: 47.38%\n",
            "\t Val. Loss: 1.390 |  Val. Acc: 37.99%\n",
            "\tTrain Loss: 1.273 | Train Acc: 43.50%\n",
            "\t Val. Loss: 1.312 |  Val. Acc: 42.56%\n",
            "\tTrain Loss: 1.202 | Train Acc: 47.44%\n",
            "\t Val. Loss: 1.391 |  Val. Acc: 37.99%\n",
            "\tTrain Loss: 1.272 | Train Acc: 43.58%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.201 | Train Acc: 47.60%\n",
            "\t Val. Loss: 1.393 |  Val. Acc: 37.90%\n",
            "\tTrain Loss: 1.275 | Train Acc: 43.67%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.203 | Train Acc: 47.61%\n",
            "\t Val. Loss: 1.388 |  Val. Acc: 38.14%\n",
            "\tTrain Loss: 1.270 | Train Acc: 43.91%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.56%\n",
            "\tTrain Loss: 1.200 | Train Acc: 47.76%\n",
            "\t Val. Loss: 1.386 |  Val. Acc: 38.11%\n",
            "\tTrain Loss: 1.267 | Train Acc: 44.09%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.69%\n",
            "\tTrain Loss: 1.198 | Train Acc: 47.93%\n",
            "\t Val. Loss: 1.384 |  Val. Acc: 38.20%\n",
            "\tTrain Loss: 1.263 | Train Acc: 44.39%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.79%\n",
            "\tTrain Loss: 1.195 | Train Acc: 48.13%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 38.50%\n",
            "\tTrain Loss: 1.260 | Train Acc: 44.62%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.193 | Train Acc: 48.18%\n",
            "\t Val. Loss: 1.381 |  Val. Acc: 38.61%\n",
            "\tTrain Loss: 1.257 | Train Acc: 44.85%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.191 | Train Acc: 48.26%\n",
            "\t Val. Loss: 1.380 |  Val. Acc: 38.66%\n",
            "\tTrain Loss: 1.254 | Train Acc: 45.07%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.189 | Train Acc: 48.37%\n",
            "\t Val. Loss: 1.380 |  Val. Acc: 38.69%\n",
            "\tTrain Loss: 1.252 | Train Acc: 45.22%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.187 | Train Acc: 48.50%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 38.74%\n",
            "\tTrain Loss: 1.249 | Train Acc: 45.36%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.68%\n",
            "\tTrain Loss: 1.185 | Train Acc: 48.57%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 38.84%\n",
            "\tTrain Loss: 1.246 | Train Acc: 45.52%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.65%\n",
            "\tTrain Loss: 1.184 | Train Acc: 48.68%\n",
            "\t Val. Loss: 1.377 |  Val. Acc: 38.98%\n",
            "\tTrain Loss: 1.244 | Train Acc: 45.72%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.66%\n",
            "\tTrain Loss: 1.182 | Train Acc: 48.79%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 39.00%\n",
            "\tTrain Loss: 1.242 | Train Acc: 45.87%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.59%\n",
            "\tTrain Loss: 1.180 | Train Acc: 48.86%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 38.94%\n",
            "\tTrain Loss: 1.240 | Train Acc: 45.99%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.68%\n",
            "\tTrain Loss: 1.179 | Train Acc: 48.94%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 38.82%\n",
            "\tTrain Loss: 1.238 | Train Acc: 46.05%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.52%\n",
            "\tTrain Loss: 1.177 | Train Acc: 49.09%\n",
            "\t Val. Loss: 1.377 |  Val. Acc: 38.90%\n",
            "\tTrain Loss: 1.236 | Train Acc: 46.07%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.60%\n",
            "\tTrain Loss: 1.175 | Train Acc: 49.20%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 39.10%\n",
            "\tTrain Loss: 1.234 | Train Acc: 46.20%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.174 | Train Acc: 49.22%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 39.13%\n",
            "\tTrain Loss: 1.233 | Train Acc: 46.29%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.48%\n",
            "\tTrain Loss: 1.172 | Train Acc: 49.32%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 39.18%\n",
            "\tTrain Loss: 1.231 | Train Acc: 46.41%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.45%\n",
            "\tTrain Loss: 1.170 | Train Acc: 49.37%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 39.14%\n",
            "\tTrain Loss: 1.229 | Train Acc: 46.54%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.51%\n",
            "\tTrain Loss: 1.169 | Train Acc: 49.48%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 39.15%\n",
            "\tTrain Loss: 1.227 | Train Acc: 46.65%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.57%\n",
            "\tTrain Loss: 1.168 | Train Acc: 49.53%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 39.18%\n",
            "\tTrain Loss: 1.225 | Train Acc: 46.75%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.49%\n",
            "\tTrain Loss: 1.166 | Train Acc: 49.61%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 39.20%\n",
            "\tTrain Loss: 1.223 | Train Acc: 46.96%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.51%\n",
            "\tTrain Loss: 1.165 | Train Acc: 49.68%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 39.20%\n",
            "\tTrain Loss: 1.221 | Train Acc: 47.04%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.60%\n",
            "\tTrain Loss: 1.163 | Train Acc: 49.76%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 39.32%\n",
            "\tTrain Loss: 1.219 | Train Acc: 47.19%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.61%\n",
            "\tTrain Loss: 1.162 | Train Acc: 49.92%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 39.40%\n",
            "\tTrain Loss: 1.217 | Train Acc: 47.25%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.54%\n",
            "\tTrain Loss: 1.160 | Train Acc: 49.96%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 39.36%\n",
            "\tTrain Loss: 1.215 | Train Acc: 47.39%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.51%\n",
            "\tTrain Loss: 1.159 | Train Acc: 50.02%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 39.30%\n",
            "\tTrain Loss: 1.213 | Train Acc: 47.43%\n",
            "\t Val. Loss: 1.323 |  Val. Acc: 42.61%\n",
            "\tTrain Loss: 1.159 | Train Acc: 50.12%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 39.31%\n",
            "\tTrain Loss: 1.210 | Train Acc: 47.53%\n",
            "\t Val. Loss: 1.324 |  Val. Acc: 42.71%\n",
            "\tTrain Loss: 1.158 | Train Acc: 50.19%\n",
            "\t Val. Loss: 1.372 |  Val. Acc: 39.31%\n",
            "\tTrain Loss: 1.207 | Train Acc: 47.61%\n",
            "\t Val. Loss: 1.325 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.156 | Train Acc: 50.27%\n",
            "\t Val. Loss: 1.372 |  Val. Acc: 39.47%\n",
            "\tTrain Loss: 1.204 | Train Acc: 47.81%\n",
            "\t Val. Loss: 1.326 |  Val. Acc: 42.95%\n",
            "\tTrain Loss: 1.154 | Train Acc: 50.42%\n",
            "\t Val. Loss: 1.372 |  Val. Acc: 39.65%\n",
            "\tTrain Loss: 1.201 | Train Acc: 47.89%\n",
            "\t Val. Loss: 1.328 |  Val. Acc: 42.81%\n",
            "\tTrain Loss: 1.151 | Train Acc: 50.51%\n",
            "\t Val. Loss: 1.372 |  Val. Acc: 39.66%\n",
            "\tTrain Loss: 1.198 | Train Acc: 48.00%\n",
            "\t Val. Loss: 1.332 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.151 | Train Acc: 50.63%\n",
            "\t Val. Loss: 1.372 |  Val. Acc: 39.57%\n",
            "\tTrain Loss: 1.195 | Train Acc: 48.40%\n",
            "\t Val. Loss: 1.334 |  Val. Acc: 42.76%\n",
            "\tTrain Loss: 1.151 | Train Acc: 50.64%\n",
            "\t Val. Loss: 1.371 |  Val. Acc: 39.66%\n",
            "\tTrain Loss: 1.191 | Train Acc: 49.05%\n",
            "\t Val. Loss: 1.337 |  Val. Acc: 42.72%\n",
            "\tTrain Loss: 1.149 | Train Acc: 50.75%\n",
            "\t Val. Loss: 1.370 |  Val. Acc: 40.09%\n",
            "\tTrain Loss: 1.189 | Train Acc: 49.41%\n",
            "\t Val. Loss: 1.337 |  Val. Acc: 42.83%\n",
            "\tTrain Loss: 1.146 | Train Acc: 51.05%\n",
            "\t Val. Loss: 1.395 |  Val. Acc: 40.30%\n",
            "\tTrain Loss: 1.207 | Train Acc: 49.08%\n",
            "\t Val. Loss: 1.389 |  Val. Acc: 39.89%\n",
            "\tTrain Loss: 1.206 | Train Acc: 47.71%\n",
            "\t Val. Loss: 1.441 |  Val. Acc: 38.87%\n",
            "\tTrain Loss: 1.270 | Train Acc: 44.88%\n",
            "\t Val. Loss: 1.383 |  Val. Acc: 39.90%\n",
            "\tTrain Loss: 1.199 | Train Acc: 47.80%\n",
            "\t Val. Loss: 1.437 |  Val. Acc: 39.29%\n",
            "\tTrain Loss: 1.263 | Train Acc: 45.25%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 39.88%\n",
            "\tTrain Loss: 1.194 | Train Acc: 48.00%\n",
            "\t Val. Loss: 1.432 |  Val. Acc: 39.55%\n",
            "\tTrain Loss: 1.257 | Train Acc: 45.57%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 40.19%\n",
            "\tTrain Loss: 1.191 | Train Acc: 48.21%\n",
            "\t Val. Loss: 1.430 |  Val. Acc: 39.77%\n",
            "\tTrain Loss: 1.253 | Train Acc: 45.86%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 40.22%\n",
            "\tTrain Loss: 1.187 | Train Acc: 48.48%\n",
            "\t Val. Loss: 1.428 |  Val. Acc: 39.86%\n",
            "\tTrain Loss: 1.249 | Train Acc: 46.11%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 40.15%\n",
            "\tTrain Loss: 1.184 | Train Acc: 48.76%\n",
            "\t Val. Loss: 1.427 |  Val. Acc: 40.06%\n",
            "\tTrain Loss: 1.245 | Train Acc: 46.34%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 40.38%\n",
            "\tTrain Loss: 1.181 | Train Acc: 48.98%\n",
            "\t Val. Loss: 1.426 |  Val. Acc: 40.34%\n",
            "\tTrain Loss: 1.241 | Train Acc: 46.70%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 40.34%\n",
            "\tTrain Loss: 1.179 | Train Acc: 49.22%\n",
            "\t Val. Loss: 1.425 |  Val. Acc: 40.34%\n",
            "\tTrain Loss: 1.238 | Train Acc: 46.84%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 40.35%\n",
            "\tTrain Loss: 1.176 | Train Acc: 49.39%\n",
            "\t Val. Loss: 1.424 |  Val. Acc: 40.47%\n",
            "\tTrain Loss: 1.235 | Train Acc: 47.00%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 40.39%\n",
            "\tTrain Loss: 1.174 | Train Acc: 49.58%\n",
            "\t Val. Loss: 1.422 |  Val. Acc: 40.44%\n",
            "\tTrain Loss: 1.233 | Train Acc: 47.17%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 40.36%\n",
            "\tTrain Loss: 1.172 | Train Acc: 49.64%\n",
            "\t Val. Loss: 1.422 |  Val. Acc: 40.44%\n",
            "\tTrain Loss: 1.230 | Train Acc: 47.37%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 40.36%\n",
            "\tTrain Loss: 1.170 | Train Acc: 49.70%\n",
            "\t Val. Loss: 1.421 |  Val. Acc: 40.47%\n",
            "\tTrain Loss: 1.227 | Train Acc: 47.49%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 40.39%\n",
            "\tTrain Loss: 1.168 | Train Acc: 49.93%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 40.46%\n",
            "\tTrain Loss: 1.225 | Train Acc: 47.69%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 40.51%\n",
            "\tTrain Loss: 1.166 | Train Acc: 49.99%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 40.50%\n",
            "\tTrain Loss: 1.223 | Train Acc: 47.81%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 40.51%\n",
            "\tTrain Loss: 1.165 | Train Acc: 50.09%\n",
            "\t Val. Loss: 1.419 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.221 | Train Acc: 47.96%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 40.47%\n",
            "\tTrain Loss: 1.163 | Train Acc: 50.09%\n",
            "\t Val. Loss: 1.418 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.219 | Train Acc: 48.00%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 40.45%\n",
            "\tTrain Loss: 1.162 | Train Acc: 50.20%\n",
            "\t Val. Loss: 1.418 |  Val. Acc: 40.78%\n",
            "\tTrain Loss: 1.217 | Train Acc: 48.19%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 40.40%\n",
            "\tTrain Loss: 1.160 | Train Acc: 50.27%\n",
            "\t Val. Loss: 1.417 |  Val. Acc: 40.72%\n",
            "\tTrain Loss: 1.215 | Train Acc: 48.28%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 40.40%\n",
            "\tTrain Loss: 1.158 | Train Acc: 50.34%\n",
            "\t Val. Loss: 1.417 |  Val. Acc: 40.75%\n",
            "\tTrain Loss: 1.213 | Train Acc: 48.40%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 40.29%\n",
            "\tTrain Loss: 1.157 | Train Acc: 50.44%\n",
            "\t Val. Loss: 1.416 |  Val. Acc: 40.84%\n",
            "\tTrain Loss: 1.211 | Train Acc: 48.49%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 40.17%\n",
            "\tTrain Loss: 1.155 | Train Acc: 50.61%\n",
            "\t Val. Loss: 1.418 |  Val. Acc: 40.87%\n",
            "\tTrain Loss: 1.210 | Train Acc: 48.53%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 40.09%\n",
            "\tTrain Loss: 1.153 | Train Acc: 50.55%\n",
            "\t Val. Loss: 1.417 |  Val. Acc: 41.12%\n",
            "\tTrain Loss: 1.207 | Train Acc: 48.64%\n",
            "\t Val. Loss: 1.381 |  Val. Acc: 39.75%\n",
            "\tTrain Loss: 1.153 | Train Acc: 50.22%\n",
            "\t Val. Loss: 1.413 |  Val. Acc: 41.04%\n",
            "\tTrain Loss: 1.205 | Train Acc: 48.34%\n",
            "\t Val. Loss: 1.383 |  Val. Acc: 39.68%\n",
            "\tTrain Loss: 1.151 | Train Acc: 50.27%\n",
            "\t Val. Loss: 1.413 |  Val. Acc: 41.04%\n",
            "\tTrain Loss: 1.201 | Train Acc: 48.42%\n",
            "\t Val. Loss: 1.380 |  Val. Acc: 39.47%\n",
            "\tTrain Loss: 1.149 | Train Acc: 50.33%\n",
            "\t Val. Loss: 1.413 |  Val. Acc: 41.23%\n",
            "\tTrain Loss: 1.198 | Train Acc: 48.52%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 39.24%\n",
            "\tTrain Loss: 1.148 | Train Acc: 50.05%\n",
            "\t Val. Loss: 1.410 |  Val. Acc: 41.35%\n",
            "\tTrain Loss: 1.202 | Train Acc: 48.11%\n",
            "\t Val. Loss: 1.377 |  Val. Acc: 39.30%\n",
            "\tTrain Loss: 1.144 | Train Acc: 50.67%\n",
            "\t Val. Loss: 1.411 |  Val. Acc: 41.42%\n",
            "\tTrain Loss: 1.198 | Train Acc: 48.28%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 39.18%\n",
            "\tTrain Loss: 1.141 | Train Acc: 50.64%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.56%\n",
            "\tTrain Loss: 1.195 | Train Acc: 48.55%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 39.30%\n",
            "\tTrain Loss: 1.138 | Train Acc: 50.77%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.54%\n",
            "\tTrain Loss: 1.192 | Train Acc: 48.68%\n",
            "\t Val. Loss: 1.380 |  Val. Acc: 39.31%\n",
            "\tTrain Loss: 1.135 | Train Acc: 50.83%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 41.62%\n",
            "\tTrain Loss: 1.189 | Train Acc: 48.80%\n",
            "\t Val. Loss: 1.381 |  Val. Acc: 39.24%\n",
            "\tTrain Loss: 1.133 | Train Acc: 50.96%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 41.71%\n",
            "\tTrain Loss: 1.186 | Train Acc: 48.89%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 39.14%\n",
            "\tTrain Loss: 1.130 | Train Acc: 51.08%\n",
            "\t Val. Loss: 1.407 |  Val. Acc: 41.90%\n",
            "\tTrain Loss: 1.183 | Train Acc: 48.98%\n",
            "\t Val. Loss: 1.383 |  Val. Acc: 39.09%\n",
            "\tTrain Loss: 1.128 | Train Acc: 51.22%\n",
            "\t Val. Loss: 1.405 |  Val. Acc: 41.81%\n",
            "\tTrain Loss: 1.180 | Train Acc: 49.17%\n",
            "\t Val. Loss: 1.386 |  Val. Acc: 39.21%\n",
            "\tTrain Loss: 1.126 | Train Acc: 51.11%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.35%\n",
            "\tTrain Loss: 1.179 | Train Acc: 49.20%\n",
            "\t Val. Loss: 1.386 |  Val. Acc: 39.18%\n",
            "\tTrain Loss: 1.126 | Train Acc: 51.29%\n",
            "\t Val. Loss: 1.410 |  Val. Acc: 41.35%\n",
            "\tTrain Loss: 1.175 | Train Acc: 49.32%\n",
            "\t Val. Loss: 1.388 |  Val. Acc: 39.27%\n",
            "\tTrain Loss: 1.124 | Train Acc: 51.35%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.44%\n",
            "\tTrain Loss: 1.172 | Train Acc: 49.46%\n",
            "\t Val. Loss: 1.388 |  Val. Acc: 39.31%\n",
            "\tTrain Loss: 1.121 | Train Acc: 51.55%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.44%\n",
            "\tTrain Loss: 1.169 | Train Acc: 49.57%\n",
            "\t Val. Loss: 1.388 |  Val. Acc: 39.34%\n",
            "\tTrain Loss: 1.119 | Train Acc: 51.72%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.53%\n",
            "\tTrain Loss: 1.167 | Train Acc: 49.63%\n",
            "\t Val. Loss: 1.389 |  Val. Acc: 39.13%\n",
            "\tTrain Loss: 1.117 | Train Acc: 51.84%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.62%\n",
            "\tTrain Loss: 1.165 | Train Acc: 49.66%\n",
            "\t Val. Loss: 1.390 |  Val. Acc: 39.21%\n",
            "\tTrain Loss: 1.115 | Train Acc: 51.89%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.59%\n",
            "\tTrain Loss: 1.163 | Train Acc: 49.79%\n",
            "\t Val. Loss: 1.390 |  Val. Acc: 39.07%\n",
            "\tTrain Loss: 1.113 | Train Acc: 51.93%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.48%\n",
            "\tTrain Loss: 1.161 | Train Acc: 49.90%\n",
            "\t Val. Loss: 1.391 |  Val. Acc: 39.09%\n",
            "\tTrain Loss: 1.112 | Train Acc: 52.09%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.49%\n",
            "\tTrain Loss: 1.158 | Train Acc: 50.00%\n",
            "\t Val. Loss: 1.392 |  Val. Acc: 38.99%\n",
            "\tTrain Loss: 1.110 | Train Acc: 52.16%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.59%\n",
            "\tTrain Loss: 1.156 | Train Acc: 50.10%\n",
            "\t Val. Loss: 1.392 |  Val. Acc: 38.96%\n",
            "\tTrain Loss: 1.108 | Train Acc: 52.22%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 41.62%\n",
            "\tTrain Loss: 1.154 | Train Acc: 50.23%\n",
            "\t Val. Loss: 1.392 |  Val. Acc: 38.95%\n",
            "\tTrain Loss: 1.107 | Train Acc: 52.31%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 41.69%\n",
            "\tTrain Loss: 1.153 | Train Acc: 50.30%\n",
            "\t Val. Loss: 1.392 |  Val. Acc: 39.04%\n",
            "\tTrain Loss: 1.106 | Train Acc: 52.25%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 41.69%\n",
            "\tTrain Loss: 1.150 | Train Acc: 50.45%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 39.04%\n",
            "\tTrain Loss: 1.106 | Train Acc: 52.24%\n",
            "\t Val. Loss: 1.407 |  Val. Acc: 41.76%\n",
            "\tTrain Loss: 1.148 | Train Acc: 50.57%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 38.95%\n",
            "\tTrain Loss: 1.104 | Train Acc: 52.30%\n",
            "\t Val. Loss: 1.407 |  Val. Acc: 41.87%\n",
            "\tTrain Loss: 1.146 | Train Acc: 50.65%\n",
            "\t Val. Loss: 1.398 |  Val. Acc: 38.95%\n",
            "\tTrain Loss: 1.102 | Train Acc: 52.42%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 41.80%\n",
            "\tTrain Loss: 1.144 | Train Acc: 50.77%\n",
            "\t Val. Loss: 1.399 |  Val. Acc: 38.98%\n",
            "\tTrain Loss: 1.100 | Train Acc: 52.51%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 41.79%\n",
            "\tTrain Loss: 1.141 | Train Acc: 50.88%\n",
            "\t Val. Loss: 1.400 |  Val. Acc: 38.81%\n",
            "\tTrain Loss: 1.098 | Train Acc: 52.58%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 41.78%\n",
            "\tTrain Loss: 1.139 | Train Acc: 50.99%\n",
            "\t Val. Loss: 1.401 |  Val. Acc: 38.85%\n",
            "\tTrain Loss: 1.097 | Train Acc: 52.64%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 41.81%\n",
            "\tTrain Loss: 1.137 | Train Acc: 51.08%\n",
            "\t Val. Loss: 1.402 |  Val. Acc: 38.87%\n",
            "\tTrain Loss: 1.095 | Train Acc: 52.76%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 41.80%\n",
            "\tTrain Loss: 1.135 | Train Acc: 51.23%\n",
            "\t Val. Loss: 1.403 |  Val. Acc: 38.89%\n",
            "\tTrain Loss: 1.093 | Train Acc: 52.84%\n",
            "\t Val. Loss: 1.405 |  Val. Acc: 41.95%\n",
            "\tTrain Loss: 1.133 | Train Acc: 51.31%\n",
            "\t Val. Loss: 1.404 |  Val. Acc: 38.87%\n",
            "\tTrain Loss: 1.092 | Train Acc: 52.95%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 41.89%\n",
            "\tTrain Loss: 1.131 | Train Acc: 51.37%\n",
            "\t Val. Loss: 1.405 |  Val. Acc: 39.01%\n",
            "\tTrain Loss: 1.091 | Train Acc: 52.96%\n",
            "\t Val. Loss: 1.404 |  Val. Acc: 41.94%\n",
            "\tTrain Loss: 1.129 | Train Acc: 51.56%\n",
            "\t Val. Loss: 1.407 |  Val. Acc: 39.10%\n",
            "\tTrain Loss: 1.089 | Train Acc: 53.07%\n",
            "\t Val. Loss: 1.407 |  Val. Acc: 41.78%\n",
            "\tTrain Loss: 1.129 | Train Acc: 51.33%\n",
            "\t Val. Loss: 1.411 |  Val. Acc: 39.15%\n",
            "\tTrain Loss: 1.094 | Train Acc: 53.14%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 40.87%\n",
            "\tTrain Loss: 1.140 | Train Acc: 50.88%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 39.34%\n",
            "\tTrain Loss: 1.128 | Train Acc: 51.81%\n",
            "\t Val. Loss: 1.472 |  Val. Acc: 40.10%\n",
            "\tTrain Loss: 1.188 | Train Acc: 50.61%\n",
            "\t Val. Loss: 1.461 |  Val. Acc: 40.38%\n",
            "\tTrain Loss: 1.143 | Train Acc: 51.47%\n",
            "\t Val. Loss: 1.469 |  Val. Acc: 40.34%\n",
            "\tTrain Loss: 1.181 | Train Acc: 51.18%\n",
            "\t Val. Loss: 1.460 |  Val. Acc: 40.49%\n",
            "\tTrain Loss: 1.140 | Train Acc: 51.59%\n",
            "\t Val. Loss: 1.469 |  Val. Acc: 40.08%\n",
            "\tTrain Loss: 1.177 | Train Acc: 51.38%\n",
            "\t Val. Loss: 1.458 |  Val. Acc: 40.54%\n",
            "\tTrain Loss: 1.137 | Train Acc: 51.70%\n",
            "\t Val. Loss: 1.467 |  Val. Acc: 40.17%\n",
            "\tTrain Loss: 1.174 | Train Acc: 51.51%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.41%\n",
            "\tTrain Loss: 1.135 | Train Acc: 51.86%\n",
            "\t Val. Loss: 1.466 |  Val. Acc: 40.20%\n",
            "\tTrain Loss: 1.172 | Train Acc: 51.61%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.50%\n",
            "\tTrain Loss: 1.133 | Train Acc: 52.01%\n",
            "\t Val. Loss: 1.465 |  Val. Acc: 40.25%\n",
            "\tTrain Loss: 1.170 | Train Acc: 51.64%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 40.51%\n",
            "\tTrain Loss: 1.131 | Train Acc: 52.12%\n",
            "\t Val. Loss: 1.464 |  Val. Acc: 40.19%\n",
            "\tTrain Loss: 1.168 | Train Acc: 51.68%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.46%\n",
            "\tTrain Loss: 1.129 | Train Acc: 52.23%\n",
            "\t Val. Loss: 1.464 |  Val. Acc: 40.19%\n",
            "\tTrain Loss: 1.166 | Train Acc: 51.78%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.56%\n",
            "\tTrain Loss: 1.128 | Train Acc: 52.38%\n",
            "\t Val. Loss: 1.463 |  Val. Acc: 40.22%\n",
            "\tTrain Loss: 1.164 | Train Acc: 51.92%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.58%\n",
            "\tTrain Loss: 1.126 | Train Acc: 52.47%\n",
            "\t Val. Loss: 1.462 |  Val. Acc: 40.30%\n",
            "\tTrain Loss: 1.162 | Train Acc: 51.97%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.49%\n",
            "\tTrain Loss: 1.125 | Train Acc: 52.50%\n",
            "\t Val. Loss: 1.462 |  Val. Acc: 40.30%\n",
            "\tTrain Loss: 1.161 | Train Acc: 52.08%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.58%\n",
            "\tTrain Loss: 1.123 | Train Acc: 52.51%\n",
            "\t Val. Loss: 1.462 |  Val. Acc: 40.41%\n",
            "\tTrain Loss: 1.159 | Train Acc: 52.15%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.122 | Train Acc: 52.60%\n",
            "\t Val. Loss: 1.462 |  Val. Acc: 40.38%\n",
            "\tTrain Loss: 1.158 | Train Acc: 52.25%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.70%\n",
            "\tTrain Loss: 1.121 | Train Acc: 52.69%\n",
            "\t Val. Loss: 1.461 |  Val. Acc: 40.30%\n",
            "\tTrain Loss: 1.157 | Train Acc: 52.29%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.74%\n",
            "\tTrain Loss: 1.119 | Train Acc: 52.74%\n",
            "\t Val. Loss: 1.461 |  Val. Acc: 40.34%\n",
            "\tTrain Loss: 1.155 | Train Acc: 52.37%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.75%\n",
            "\tTrain Loss: 1.118 | Train Acc: 52.82%\n",
            "\t Val. Loss: 1.461 |  Val. Acc: 40.35%\n",
            "\tTrain Loss: 1.154 | Train Acc: 52.45%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.79%\n",
            "\tTrain Loss: 1.117 | Train Acc: 52.89%\n",
            "\t Val. Loss: 1.460 |  Val. Acc: 40.29%\n",
            "\tTrain Loss: 1.153 | Train Acc: 52.53%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.75%\n",
            "\tTrain Loss: 1.116 | Train Acc: 52.97%\n",
            "\t Val. Loss: 1.460 |  Val. Acc: 40.38%\n",
            "\tTrain Loss: 1.152 | Train Acc: 52.56%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.69%\n",
            "\tTrain Loss: 1.115 | Train Acc: 53.02%\n",
            "\t Val. Loss: 1.460 |  Val. Acc: 40.53%\n",
            "\tTrain Loss: 1.151 | Train Acc: 52.57%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.114 | Train Acc: 53.12%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 40.46%\n",
            "\tTrain Loss: 1.149 | Train Acc: 52.60%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.113 | Train Acc: 53.14%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 40.50%\n",
            "\tTrain Loss: 1.148 | Train Acc: 52.64%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.66%\n",
            "\tTrain Loss: 1.112 | Train Acc: 53.15%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 40.49%\n",
            "\tTrain Loss: 1.147 | Train Acc: 52.67%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.72%\n",
            "\tTrain Loss: 1.112 | Train Acc: 53.18%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 40.51%\n",
            "\tTrain Loss: 1.146 | Train Acc: 52.70%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.70%\n",
            "\tTrain Loss: 1.111 | Train Acc: 53.21%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 40.55%\n",
            "\tTrain Loss: 1.146 | Train Acc: 52.72%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.69%\n",
            "\tTrain Loss: 1.110 | Train Acc: 53.22%\n",
            "\t Val. Loss: 1.458 |  Val. Acc: 40.51%\n",
            "\tTrain Loss: 1.145 | Train Acc: 52.79%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.65%\n",
            "\tTrain Loss: 1.109 | Train Acc: 53.24%\n",
            "\t Val. Loss: 1.458 |  Val. Acc: 40.61%\n",
            "\tTrain Loss: 1.144 | Train Acc: 52.84%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.59%\n",
            "\tTrain Loss: 1.109 | Train Acc: 53.32%\n",
            "\t Val. Loss: 1.458 |  Val. Acc: 40.69%\n",
            "\tTrain Loss: 1.143 | Train Acc: 52.88%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.55%\n",
            "\tTrain Loss: 1.108 | Train Acc: 53.37%\n",
            "\t Val. Loss: 1.458 |  Val. Acc: 40.74%\n",
            "\tTrain Loss: 1.142 | Train Acc: 52.89%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.62%\n",
            "\tTrain Loss: 1.107 | Train Acc: 53.42%\n",
            "\t Val. Loss: 1.458 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.141 | Train Acc: 52.90%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.107 | Train Acc: 53.44%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.66%\n",
            "\tTrain Loss: 1.140 | Train Acc: 52.92%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.106 | Train Acc: 53.44%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.72%\n",
            "\tTrain Loss: 1.139 | Train Acc: 52.99%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.74%\n",
            "\tTrain Loss: 1.105 | Train Acc: 53.48%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.78%\n",
            "\tTrain Loss: 1.139 | Train Acc: 53.07%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.70%\n",
            "\tTrain Loss: 1.105 | Train Acc: 53.49%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.74%\n",
            "\tTrain Loss: 1.138 | Train Acc: 53.10%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.70%\n",
            "\tTrain Loss: 1.104 | Train Acc: 53.52%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.75%\n",
            "\tTrain Loss: 1.137 | Train Acc: 53.14%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.72%\n",
            "\tTrain Loss: 1.103 | Train Acc: 53.55%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.75%\n",
            "\tTrain Loss: 1.136 | Train Acc: 53.14%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.66%\n",
            "\tTrain Loss: 1.103 | Train Acc: 53.56%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.75%\n",
            "\tTrain Loss: 1.136 | Train Acc: 53.17%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.65%\n",
            "\tTrain Loss: 1.102 | Train Acc: 53.59%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.135 | Train Acc: 53.19%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.58%\n",
            "\tTrain Loss: 1.102 | Train Acc: 53.63%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.75%\n",
            "\tTrain Loss: 1.134 | Train Acc: 53.20%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.59%\n",
            "\tTrain Loss: 1.101 | Train Acc: 53.69%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.71%\n",
            "\tTrain Loss: 1.133 | Train Acc: 53.22%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.59%\n",
            "\tTrain Loss: 1.101 | Train Acc: 53.71%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.69%\n",
            "\tTrain Loss: 1.133 | Train Acc: 53.27%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.50%\n",
            "\tTrain Loss: 1.100 | Train Acc: 53.72%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.132 | Train Acc: 53.32%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.59%\n",
            "\tTrain Loss: 1.099 | Train Acc: 53.79%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.131 | Train Acc: 53.34%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 40.61%\n",
            "\tTrain Loss: 1.099 | Train Acc: 53.79%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.65%\n",
            "\tTrain Loss: 1.131 | Train Acc: 53.39%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.65%\n",
            "\tTrain Loss: 1.098 | Train Acc: 53.82%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.130 | Train Acc: 53.41%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.098 | Train Acc: 53.86%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.129 | Train Acc: 53.41%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.54%\n",
            "\tTrain Loss: 1.097 | Train Acc: 53.90%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.70%\n",
            "\tTrain Loss: 1.129 | Train Acc: 53.43%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.50%\n",
            "\tTrain Loss: 1.097 | Train Acc: 53.90%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.128 | Train Acc: 53.54%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.60%\n",
            "\tTrain Loss: 1.096 | Train Acc: 53.97%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.70%\n",
            "\tTrain Loss: 1.127 | Train Acc: 53.57%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.66%\n",
            "\tTrain Loss: 1.096 | Train Acc: 54.00%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.72%\n",
            "\tTrain Loss: 1.127 | Train Acc: 53.59%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 40.71%\n",
            "\tTrain Loss: 1.095 | Train Acc: 54.01%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.70%\n",
            "\tTrain Loss: 1.126 | Train Acc: 53.67%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.65%\n",
            "\tTrain Loss: 1.095 | Train Acc: 54.01%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.67%\n",
            "\tTrain Loss: 1.126 | Train Acc: 53.63%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.74%\n",
            "\tTrain Loss: 1.095 | Train Acc: 54.05%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 40.71%\n",
            "\tTrain Loss: 1.125 | Train Acc: 53.67%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.66%\n",
            "\tTrain Loss: 1.094 | Train Acc: 54.07%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.79%\n",
            "\tTrain Loss: 1.125 | Train Acc: 53.70%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.60%\n",
            "\tTrain Loss: 1.094 | Train Acc: 54.05%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 40.76%\n",
            "\tTrain Loss: 1.124 | Train Acc: 53.69%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.58%\n",
            "\tTrain Loss: 1.093 | Train Acc: 54.10%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 40.83%\n",
            "\tTrain Loss: 1.124 | Train Acc: 53.76%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.51%\n",
            "\tTrain Loss: 1.093 | Train Acc: 54.13%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 40.78%\n",
            "\tTrain Loss: 1.123 | Train Acc: 53.80%\n",
            "\t Val. Loss: 1.452 |  Val. Acc: 40.56%\n",
            "\tTrain Loss: 1.092 | Train Acc: 54.18%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.86%\n",
            "\tTrain Loss: 1.122 | Train Acc: 53.85%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.49%\n",
            "\tTrain Loss: 1.092 | Train Acc: 54.19%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.78%\n",
            "\tTrain Loss: 1.122 | Train Acc: 53.83%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.50%\n",
            "\tTrain Loss: 1.091 | Train Acc: 54.18%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.78%\n",
            "\tTrain Loss: 1.121 | Train Acc: 53.84%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.51%\n",
            "\tTrain Loss: 1.091 | Train Acc: 54.21%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.78%\n",
            "\tTrain Loss: 1.121 | Train Acc: 53.85%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.49%\n",
            "\tTrain Loss: 1.091 | Train Acc: 54.23%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 40.81%\n",
            "\tTrain Loss: 1.120 | Train Acc: 53.90%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.47%\n",
            "\tTrain Loss: 1.090 | Train Acc: 54.25%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 40.85%\n",
            "\tTrain Loss: 1.120 | Train Acc: 53.96%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.47%\n",
            "\tTrain Loss: 1.090 | Train Acc: 54.29%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.87%\n",
            "\tTrain Loss: 1.119 | Train Acc: 54.00%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.47%\n",
            "\tTrain Loss: 1.089 | Train Acc: 54.33%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 40.78%\n",
            "\tTrain Loss: 1.119 | Train Acc: 53.99%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 40.47%\n",
            "\tTrain Loss: 1.089 | Train Acc: 54.36%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.76%\n",
            "\tTrain Loss: 1.118 | Train Acc: 54.02%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.66%\n",
            "\tTrain Loss: 1.088 | Train Acc: 54.43%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.80%\n",
            "\tTrain Loss: 1.118 | Train Acc: 54.03%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.64%\n",
            "\tTrain Loss: 1.088 | Train Acc: 54.47%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.84%\n",
            "\tTrain Loss: 1.117 | Train Acc: 54.06%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.55%\n",
            "\tTrain Loss: 1.087 | Train Acc: 54.51%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.84%\n",
            "\tTrain Loss: 1.117 | Train Acc: 54.07%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.58%\n",
            "\tTrain Loss: 1.087 | Train Acc: 54.58%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.81%\n",
            "\tTrain Loss: 1.116 | Train Acc: 54.09%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.59%\n",
            "\tTrain Loss: 1.087 | Train Acc: 54.61%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.87%\n",
            "\tTrain Loss: 1.116 | Train Acc: 54.03%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.71%\n",
            "\tTrain Loss: 1.086 | Train Acc: 54.61%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.89%\n",
            "\tTrain Loss: 1.115 | Train Acc: 54.08%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.74%\n",
            "\tTrain Loss: 1.086 | Train Acc: 54.60%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.79%\n",
            "\tTrain Loss: 1.115 | Train Acc: 54.12%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.76%\n",
            "\tTrain Loss: 1.085 | Train Acc: 54.61%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.86%\n",
            "\tTrain Loss: 1.115 | Train Acc: 54.17%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.74%\n",
            "\tTrain Loss: 1.085 | Train Acc: 54.69%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.84%\n",
            "\tTrain Loss: 1.114 | Train Acc: 54.18%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.71%\n",
            "\tTrain Loss: 1.085 | Train Acc: 54.71%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.83%\n",
            "\tTrain Loss: 1.114 | Train Acc: 54.21%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.62%\n",
            "\tTrain Loss: 1.084 | Train Acc: 54.75%\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 40.79%\n",
            "\tTrain Loss: 1.113 | Train Acc: 54.17%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 40.64%\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "torch.manual_seed(0)\n",
        "from torch.optim import SGD\n",
        "from numpy import vstack\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def accuracy(probs, target):\n",
        "  winners = probs.argmax(dim=1)+1  # getting the class with max probability and adding 1 since pytorch has indices 0 to n\n",
        "  target=target+1\n",
        "  corrects = (winners == target)\n",
        "  accuracy = corrects.sum().float() / float(target.size(0))\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def train_model(train_dl, y_tr_simple, model):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  criterion = CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "  optimizer.zero_grad()\n",
        "  output = model(train_dl)\n",
        "  target=torch.from_numpy(y_tr_simple)\n",
        "  target=target-1\n",
        "  loss = criterion(output, target)\n",
        "  acc = accuracy(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  epoch_loss += loss.item()\n",
        "  epoch_acc += acc.item()\n",
        "  return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate_model(test_dl, y_ts, model):\n",
        "   epoch_loss = 0\n",
        "   epoch_acc = 0\n",
        "   criterion = CrossEntropyLoss()\n",
        "   with torch.no_grad():\n",
        "        yhat = model(test_dl)\n",
        "        target = torch.from_numpy(y_ts)\n",
        "        target=target-1\n",
        "        loss = criterion(yhat, target)\n",
        "        acc = accuracy(yhat, target)   \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        return epoch_loss, epoch_acc \n",
        "\n",
        "\n",
        "concatMLP_model = concatMLP(3000)\n",
        "X_train = torch.from_numpy(X_tr_simple_concat)\n",
        "X_val = torch.from_numpy(X_val_simple_concat)\n",
        "X_test = torch.from_numpy(X_ts_simple_concat)\n",
        "X_train = X_train.float()\n",
        "X_val=X_val.float()\n",
        "X_test=X_test.float()\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(500):\n",
        "   train_loss, train_acc = train_model(X_train, y_tr_simple_concat, concatMLP_model)\n",
        "   valid_loss, valid_acc = evaluate_model(X_val, y_val_simple_concat, concatMLP_model)\n",
        "   if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(concatMLP_model.state_dict(), 'saved_weights'+'_'+'concatMLP_model'+'.pt')\n",
        "   print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "   print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7zhTPI3G6ZK",
        "outputId": "79498ae3-6bc9-44b3-ea57-0b5775d33365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " For MLP Feedforward taking concatenation of vectors: Test Acc: 42.41%\n"
          ]
        }
      ],
      "source": [
        "concatMLP_model.load_state_dict(torch.load(os.path.join(\"saved_weights_concatMLP_model.pt\")))\n",
        "test_loss, test_acc = evaluate_model(X_test, y_ts_simple_concat, concatMLP_model)\n",
        "print(f' For MLP Feedforward taking concatenation of vectors: Test Acc: {test_acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From above we can conclude that, for feedforward, average as a feature is performing better than word concatenation. This is becasue we are only concatenating the first 10 word vectors. So there is a  possibility that more important information or context is present after 10 words and we are missing that information. For ex, I bought this ring for my mother on her birthday as a gift and I really loved the ring and would definitely buy again; beautiful color, amazing quality etc. In this ex, all the positive context is present after 10 words so we might classify this as neutral but is actually positive. But for avg, we have given weight to all the words. Hence, avg might be performing better. Also w.r.t models SVM is performing better than feedforward by a margin but feedforward is performing better than simple perceptron. Also, we can further improve feedforward by adding more epochs and parameter tuning to cross SVM**"
      ],
      "metadata": {
        "id": "W6ZeY_Eu82jx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpelWaYPdhmk"
      },
      "source": [
        "**Recurrent Neural Networks  from scratch without using nn.rnn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY7cVHjqxHK-"
      },
      "outputs": [],
      "source": [
        "x_tr=[]\n",
        "y_tr=[]\n",
        "x_ts=[]\n",
        "y_ts=[]\n",
        "for i in range(len(train_data)):\n",
        "  count=0\n",
        "  doc=[]\n",
        "  for word in vars(train_data[i])['review_body']:\n",
        "    if count>=20:\n",
        "      break\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "    count+=1\n",
        "  for k in range(count,20):\n",
        "    doc.append(np.zeros(300))\n",
        "  x_tr.append(doc)\n",
        "  y_tr.append(vars(train_data[i])['star_rating'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7AFcFK1wiI1"
      },
      "outputs": [],
      "source": [
        "x_val=[]\n",
        "y_val=[]\n",
        "for i in range(len(validation_data)):\n",
        "  count=0\n",
        "  doc=[]\n",
        "  for word in vars(validation_data[i])['review_body']:\n",
        "    if count>=20:\n",
        "      break\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "    count+=1\n",
        "  for k in range(count,20):\n",
        "    doc.append(np.zeros(300))\n",
        "  x_val.append(doc)\n",
        "  y_val.append(vars(validation_data[i])['star_rating'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfXIRa54MN6P"
      },
      "outputs": [],
      "source": [
        "for i in range(len(testing_data)):\n",
        "  count=0\n",
        "  doc=[]\n",
        "  for word in vars(testing_data[i])['review_body']:\n",
        "    if count>=20:\n",
        "      break\n",
        "    if word in model.vocab:\n",
        "      doc.append(model[word])\n",
        "    else:\n",
        "      doc.append(np.zeros(300))\n",
        "    count+=1\n",
        "  for k in range(count,20):\n",
        "    doc.append(np.zeros(300))\n",
        "  x_ts.append(doc)\n",
        "  y_ts.append(vars(testing_data[i])['star_rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm1JOKPZNlLU"
      },
      "outputs": [],
      "source": [
        "X_tr_simple_RNN = np.array(x_tr)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_simple_RNN = np.array(x_val)"
      ],
      "metadata": {
        "id": "BGuaZRvEHG6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ts_simple_RNN = np.array(x_ts)"
      ],
      "metadata": {
        "id": "b8gmh-E5HJcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr_simple_RNN = np.array(y_tr)"
      ],
      "metadata": {
        "id": "vuz_nvZuHLSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9gMnK8G4m8W"
      },
      "outputs": [],
      "source": [
        "y_val_simple_RNN = np.array(y_val)\n",
        "y_ts_simple_RNN = np.array(y_ts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#weights = torch.FloatTensor(Text.vocab.vectors)\n",
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(RNN,self).__init__()\n",
        "        self.hidden_dim=20\n",
        "        self.n_layers=1\n",
        "        self.i2h = torch.nn.Linear(input_size + self.hidden_dim, self.hidden_dim)\n",
        "        self.i2o = torch.nn.Linear(input_size + self.hidden_dim, 5)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "        #self.rnn = torch.nn.RNN(input_size, self.hidden_dim, self.n_layers, batch_first=True, nonlinearity=\"relu\")\n",
        "        #self.linear  = torch.nn.Linear(self.hidden_dim, 5)   # each word represented by 300 dimensions \n",
        "   \n",
        "    def init_hidden(self,batch_size):\n",
        "      return torch.zeros(1, self.hidden_dim)\n",
        "      # hidden=torch.zeros(self.n_layers,batch_size,self.hidden_dim)\n",
        "      # return hidden\n",
        "   \n",
        "   \n",
        "    def forward(self, X, hidden): \n",
        "        X = X.view(1, -1)\n",
        "        combined = torch.cat((X, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "        \n",
        "        # X = X.view(1, -1)\n",
        "        # X = X.unsqueeze(0)\n",
        "        # hidden=self.init_hidden(1) \n",
        "        # X,hidden = self.rnn(X,hidden)\n",
        "        # X = X.view(X.size(0), self.hidden_dim)\n",
        "        # X = self.linear(X)\n",
        "        # return X,hidden"
      ],
      "metadata": {
        "id": "EdDQ9SrQ-4B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from numpy import vstack\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def accuracy(probs, target):\n",
        "  winners = probs.argmax(dim=1)+1  # getting the class with max probability and adding 1 since pytorch has indices 0 to n\n",
        "  target=target+1\n",
        "  if winners==target:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        " \n",
        "def train_model(train_dl, y_tr_simple, model):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  #criterion = CrossEntropyLoss()\n",
        "  criterion = torch.nn.NLLLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "  optimizer.zero_grad()\n",
        "  hidden = model.init_hidden(1)\n",
        "  for vector in train_dl:  # taking one word at a time\n",
        "    output,hidden = model(vector,hidden)\n",
        "  y_tr_simple=y_tr_simple-1\n",
        "  loss = criterion(output, torch.tensor([y_tr_simple]))\n",
        "  acc = accuracy(output, torch.tensor([y_tr_simple]))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  return loss, acc\n",
        "\n",
        "\n",
        "def evaluate_model(test_dl, y_ts, model):\n",
        "   epoch_loss = 0\n",
        "   epoch_acc = 0\n",
        "   criterion = CrossEntropyLoss()\n",
        "   hidden = model.init_hidden(1)\n",
        "   for test_vector in test_dl:  # passing word by word\n",
        "          yhat,hidden = model(test_vector,hidden)\n",
        "   y_ts=y_ts- 1\n",
        "   loss = criterion(yhat, torch.tensor([y_ts]))\n",
        "   acc = accuracy(yhat, torch.tensor([y_ts]))\n",
        "\n",
        "   return loss,acc\n",
        "\n",
        "rnn_model = RNN(300)\n",
        "X_train = torch.from_numpy(X_tr_simple_RNN)\n",
        "X_val = torch.from_numpy(X_val_simple_RNN)\n",
        "testing_data = torch.from_numpy(X_ts_simple_RNN)\n",
        "X_train = X_train.float()\n",
        "X_val=X_val.float()\n",
        "X_test=testing_data.float()\n",
        "count=0\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(10):\n",
        "   count+=1\n",
        "   tr_loss=0\n",
        "   tr_acc=0\n",
        "   val_loss=0\n",
        "   val_acc=0 \n",
        "   for i, x in enumerate(X_train):\n",
        "      train_loss, train_acc = train_model(x, y_tr_simple_RNN[i], rnn_model)\n",
        "      tr_loss+=train_loss\n",
        "      tr_acc+=train_acc\n",
        "   for j, y in enumerate(X_val):\n",
        "      valid_loss, valid_acc = evaluate_model(y, y_val_simple_RNN[j], rnn_model)\n",
        "      val_loss+=valid_loss\n",
        "      val_acc+=valid_acc\n",
        "   if val_loss < best_valid_loss:\n",
        "              best_valid_loss = val_loss\n",
        "              torch.save(rnn_model.state_dict(), 'saved_weights'+'_'+'RNN'+'.pt')\n",
        "   print (\"Epoch: \"+str(count))\n",
        "   print(f'\\tTrain Loss: {(train_loss * 100)/len(X_train):.3f} | Train Acc: {(tr_acc * 100)/len(X_train):.2f}%')\n",
        "   print(f'\\t Val. Loss: {(valid_loss * 100)/len(X_val):.3f} |  Val. Acc: {(val_acc * 100)/len(X_val):.2f}%')\n",
        "   print (\"-----------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvjxHwaN-5Eo",
        "outputId": "bba65464-46aa-4466-a34e-43982ac9e3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\tTrain Loss: 0.003 | Train Acc: 21.05%\n",
            "\t Val. Loss: 0.019 |  Val. Acc: 23.05%\n",
            "-----------------------------------\n",
            "Epoch: 2\n",
            "\tTrain Loss: 0.009 | Train Acc: 22.96%\n",
            "\t Val. Loss: 0.038 |  Val. Acc: 22.21%\n",
            "-----------------------------------\n",
            "Epoch: 3\n",
            "\tTrain Loss: 0.000 | Train Acc: 24.81%\n",
            "\t Val. Loss: 0.087 |  Val. Acc: 22.68%\n",
            "-----------------------------------\n",
            "Epoch: 4\n",
            "\tTrain Loss: 0.004 | Train Acc: 27.85%\n",
            "\t Val. Loss: 0.072 |  Val. Acc: 23.60%\n",
            "-----------------------------------\n",
            "Epoch: 5\n",
            "\tTrain Loss: 0.000 | Train Acc: 25.91%\n",
            "\t Val. Loss: 0.165 |  Val. Acc: 23.09%\n",
            "-----------------------------------\n",
            "Epoch: 6\n",
            "\tTrain Loss: 0.000 | Train Acc: 28.23%\n",
            "\t Val. Loss: 0.226 |  Val. Acc: 22.16%\n",
            "-----------------------------------\n",
            "Epoch: 7\n",
            "\tTrain Loss: 0.002 | Train Acc: 27.95%\n",
            "\t Val. Loss: 0.158 |  Val. Acc: 22.89%\n",
            "-----------------------------------\n",
            "Epoch: 8\n",
            "\tTrain Loss: 0.000 | Train Acc: 26.96%\n",
            "\t Val. Loss: 0.207 |  Val. Acc: 23.36%\n",
            "-----------------------------------\n",
            "Epoch: 9\n",
            "\tTrain Loss: 0.000 | Train Acc: 27.56%\n",
            "\t Val. Loss: 0.177 |  Val. Acc: 22.34%\n",
            "-----------------------------------\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.000 | Train Acc: 27.68%\n",
            "\t Val. Loss: 0.141 |  Val. Acc: 20.93%\n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: I ran my code on only 10 epochs due to session crash and infrastructure issues. I have also experimented sending entire sequence to RNN at the end of the notebook. Results are decent with whole sequence.**"
      ],
      "metadata": {
        "id": "UQsTT7klOBX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.load_state_dict(torch.load(os.path.join(\"saved_weights_RNN.pt\")))\n",
        "t_loss=0\n",
        "t_acc=0\n",
        "for j, t in enumerate(X_test):\n",
        "      test_loss, test_acc = evaluate_model(t, y_ts_simple_RNN[j], rnn_model)\n",
        "      t_loss+=test_loss\n",
        "      t_acc+=test_acc\n",
        "print(f' For RNN: Test Acc: {(t_acc * 100)/len(X_test):.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIFA7Nqh_wA4",
        "outputId": "768d40c3-de23-4ccf-d207-3b3677a57766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " For RNN: Test Acc: 22.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on above, we can say that feedforward models are giving much better results than RNN. One reason for this can be because of zero padding. If there are multiple reviews with length  < 20 which we are doing for RNN, this can lead to a lot of zero vectors being fed. This can lead to a poor performance**"
      ],
      "metadata": {
        "id": "xHRMLUUXuRoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Since, we are not being judged on accuracy, I have not performed significant tuning. Also, the data is small due to session crash. Results will definitely get better with more data.**"
      ],
      "metadata": {
        "id": "gsL2FaAqOSjb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsrLO8amQJRU"
      },
      "source": [
        "**Gated Recurrent Unit Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDnZk03_zWL1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#weights = torch.FloatTensor(Text.vocab.vectors)\n",
        "class GRNN(torch.nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(GRNN,self).__init__()\n",
        "        self.hidden_dim=20\n",
        "        self.n_layers=1\n",
        "\n",
        "        # self.i2h = torch.nn.Linear(input_size + self.hidden_dim, self.hidden_dim)\n",
        "        # self.i2o = torch.nn.Linear(input_size + self.hidden_dim, 5)\n",
        "        # self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.grnn = torch.nn.GRU(input_size, self.hidden_dim, self.n_layers, batch_first=True)\n",
        "        self.linear  = torch.nn.Linear(self.hidden_dim, 5)   # each word represented by 300 dimensions \n",
        "        self.relu = torch.nn.ReLU()\n",
        "        #self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "   \n",
        "    def init_hidden(self,batch_size):\n",
        "      #hidden=torch.zeros(self.n_layers,batch_size,self.hidden_dim).requires_grad_()\n",
        "      #return hidden\n",
        "      return torch.zeros(1, 1, self.hidden_dim)\n",
        "   \n",
        "   \n",
        "    def forward(self, X, hidden): \n",
        "        X = X.view(1, -1)\n",
        "        X = X.unsqueeze(0)\n",
        "        hidden=self.init_hidden(1) \n",
        "        X,hidden = self.grnn(X,hidden)\n",
        "        X = X.view(X.size(0), self.hidden_dim)\n",
        "        X = self.linear(self.relu(X))\n",
        "        #X = self.softmax(X)\n",
        "        return X,hidden\n",
        "        # print (hidden.shape)\n",
        "        # combined = torch.cat((X, hidden),1)\n",
        "        # hidden = self.i2h(combined)\n",
        "        # output = self.i2o(combined)\n",
        "        # output = self.softmax(output)\n",
        "        # return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PM_KZF1WCUmv",
        "outputId": "99290f66-8d53-4292-9875-f6267b7f6dd6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "\tTrain Loss: 0.004 | Train Acc: 19.96%\n",
            "\t Val. Loss: 0.026 |  Val. Acc: 20.10%\n",
            "-----------------------------------\n",
            "Epoch: 2\n",
            "\tTrain Loss: 0.006 | Train Acc: 20.61%\n",
            "\t Val. Loss: 0.019 |  Val. Acc: 20.27%\n",
            "-----------------------------------\n",
            "Epoch: 3\n",
            "\tTrain Loss: 0.003 | Train Acc: 20.31%\n",
            "\t Val. Loss: 0.042 |  Val. Acc: 20.12%\n",
            "-----------------------------------\n",
            "Epoch: 4\n",
            "\tTrain Loss: 0.013 | Train Acc: 20.75%\n",
            "\t Val. Loss: 0.063 |  Val. Acc: 19.86%\n",
            "-----------------------------------\n",
            "Epoch: 5\n",
            "\tTrain Loss: 0.008 | Train Acc: 20.50%\n",
            "\t Val. Loss: 0.072 |  Val. Acc: 19.90%\n",
            "-----------------------------------\n",
            "Epoch: 6\n",
            "\tTrain Loss: 0.008 | Train Acc: 20.73%\n",
            "\t Val. Loss: 0.029 |  Val. Acc: 20.34%\n",
            "-----------------------------------\n",
            "Epoch: 7\n",
            "\tTrain Loss: 0.010 | Train Acc: 20.93%\n",
            "\t Val. Loss: 0.035 |  Val. Acc: 19.81%\n",
            "-----------------------------------\n",
            "Epoch: 8\n",
            "\tTrain Loss: 0.014 | Train Acc: 20.84%\n",
            "\t Val. Loss: 0.095 |  Val. Acc: 19.88%\n",
            "-----------------------------------\n",
            "Epoch: 9\n",
            "\tTrain Loss: 0.014 | Train Acc: 20.97%\n",
            "\t Val. Loss: 0.069 |  Val. Acc: 20.21%\n",
            "-----------------------------------\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.017 | Train Acc: 20.98%\n",
            "\t Val. Loss: 0.073 |  Val. Acc: 19.93%\n",
            "-----------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from numpy import vstack\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def accuracy(probs, target):\n",
        "  winners = probs.argmax(dim=1)+1  # getting the class with max probability and adding 1 since pytorch has indices 0 to n\n",
        "  target=target+1\n",
        "  if winners==target:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  # corrects = (winners == target)\n",
        "  # accuracy = corrects.sum().float()\n",
        "  # return accuracy\n",
        "\n",
        "\n",
        "def train_model(train_dl, y_tr_simple, model):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  criterion = CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "  optimizer.zero_grad()\n",
        "  hidden = model.init_hidden(1)\n",
        "  for vector in train_dl:  # taking one word at a time\n",
        "    output,hidden = model(vector,hidden)\n",
        "  y_tr_simple=y_tr_simple-1\n",
        "  loss = criterion(output, torch.tensor([y_tr_simple]))\n",
        "  acc = accuracy(output, torch.tensor([y_tr_simple]))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  #epoch_loss += loss.item()\n",
        "  #epoch_acc += acc.item()\n",
        "  #return epoch_loss, epoch_acc\n",
        "  return loss, acc\n",
        "\n",
        "\n",
        "def evaluate_model(test_dl, y_ts, model):\n",
        "   epoch_loss = 0\n",
        "   epoch_acc = 0\n",
        "   criterion = CrossEntropyLoss()\n",
        "   hidden = model.init_hidden(1)\n",
        "   for test_vector in test_dl:\n",
        "          yhat,hidden = model(test_vector,hidden)\n",
        "   y_ts=y_ts- 1\n",
        "   loss = criterion(yhat, torch.tensor([y_ts]))\n",
        "   acc = accuracy(yhat, torch.tensor([y_ts]))\n",
        "   #epoch_loss += loss.item()\n",
        "   #epoch_acc += acc.item()\n",
        "   #return epoch_loss, epoch_acc \n",
        "   return loss,acc\n",
        "\n",
        "\n",
        "# grnn_model = GRNN(6000)\n",
        "grnn_model = GRNN(300)\n",
        "X_train = torch.from_numpy(X_tr_simple_RNN)\n",
        "X_val = torch.from_numpy(X_val_simple_RNN)\n",
        "testing_data = torch.from_numpy(X_ts_simple_RNN)\n",
        "X_train = X_train.float()\n",
        "X_val=X_val.float()\n",
        "X_test=testing_data.float()\n",
        "count=0\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(10):\n",
        "   count+=1\n",
        "   tr_loss=0\n",
        "   tr_acc=0\n",
        "   val_loss=0\n",
        "   val_acc=0 \n",
        "   for i, x in enumerate(X_train):\n",
        "      train_loss, train_acc = train_model(x, y_tr_simple_RNN[i], grnn_model)\n",
        "      tr_loss+=train_loss\n",
        "      tr_acc+=train_acc\n",
        "   for j, y in enumerate(X_val):\n",
        "      valid_loss, valid_acc = evaluate_model(y, y_val_simple_RNN[j], grnn_model)\n",
        "      val_loss+=valid_loss\n",
        "      val_acc+=valid_acc\n",
        "   if val_loss < best_valid_loss:\n",
        "              best_valid_loss = val_loss\n",
        "              torch.save(grnn_model.state_dict(), 'saved_weights'+'_'+'GRNN'+'.pt')\n",
        "   print (\"Epoch: \"+str(count))\n",
        "   print(f'\\tTrain Loss: {(train_loss * 100) /len(X_train):.3f} | Train Acc: {(tr_acc * 100)/len(X_train):.2f}%')\n",
        "   print(f'\\t Val. Loss: {(valid_loss * 100) /len(X_val):.3f} |  Val. Acc: {(val_acc * 100)/len(X_val):.2f}%')\n",
        "   print (\"-----------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xZPAIC9klPvQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5f4d2427-78f1-43b6-ea88-b6d7c5ae8067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " For Gated RNN: Test Acc: 20.14%\n"
          ]
        }
      ],
      "source": [
        "grnn_model.load_state_dict(torch.load(os.path.join(\"saved_weights_GRNN.pt\")))\n",
        "t_loss=0\n",
        "t_acc=0\n",
        "for j, t in enumerate(X_test):\n",
        "      test_loss, test_acc = evaluate_model(t, y_ts_simple_RNN[j], grnn_model)\n",
        "      t_loss+=test_loss\n",
        "      t_acc+=test_acc\n",
        "print(f' For Gated RNN: Test Acc: {(t_acc * 100)/len(X_test):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on above, we can say that RNN performs better than gated by a small margin. This can be since RNN works well when the sequence is small and there might be cases when the reviews are short. Also, I haven't performed significant tuning due to insufficient computational power. Gated RNN might work well after fine tuning.**"
      ],
      "metadata": {
        "id": "0ZXLN3O5tstB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
        "\n",
        "https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks#3\n",
        "\n",
        "https://galhever.medium.com/sentiment-analysis-with-pytorch-part-4-lstm-bilstm-model-84447f6c4525\n",
        "\n",
        "https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
        "\n",
        "https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/"
      ],
      "metadata": {
        "id": "s3dHCfDiwkRA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}